{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNFz3Xx/vp88/KvdGiVdI/L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SY-256/llms-from-scratch/blob/main/notebooks/ch02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter2: Working with Text Data"
      ],
      "metadata": {
        "id": "bO9Q881eHAKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 テキストをトークン化する"
      ],
      "metadata": {
        "id": "AkWUZIB2N8jC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "print(\"torch version: \", version(\"torch\"))\n",
        "print(\"tiktoken version: \", version(\"tiktoken\"))"
      ],
      "metadata": {
        "id": "wJAmnZV3H4CD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "if not os.path.exists(\"the-verdict.txt\"):\n",
        "    url = (\n",
        "        \"https://raw.githubusercontent.com/rasbt/\"\n",
        "        \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "        \"the-verdict.txt\"\n",
        "    )\n",
        "    file_path = \"the-verdict.txt\"\n",
        "\n",
        "    response = requests.get(url, timeout=30)\n",
        "    response.raise_for_status()\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        f.write(response.content)\n"
      ],
      "metadata": {
        "id": "K_a7i3EXIIzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print(\"Total number of character. \", len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "id": "EFyy_yoDIu1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 空白で文字列分割\n",
        "import re\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "aYN6uRbXJJ2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- テキストをすべて小文字にするのはやめる -> 大文字はLLMが固有名詞と普通名詞を区別し、文の構造を理解し、大文字を正しく使ったテキストの生成を学習するのに役立つため安易に小文字に変換しない"
      ],
      "metadata": {
        "id": "ToTMVqPZJnd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ホワイトスペース+コンマ、ピリオドでの正規表現分割\n",
        "result = re.split(r'([,.]|\\s)', text)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "18RZJJTpKK05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ホワイトスペース不要なので除く\n",
        "item = re.split(r'([,.]|\\s)', text)\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "id": "tc5yrgARKa0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ホワイトスペースを取り除くか／取り除かないかはアプリケーションとその要件によって変わってくので、一概にすべて取り除けばよいという訳でもない\n",
        "- テキストの正確な構造に敏感なモデル（Pythonコード生成）であればホワイトスペースは残すべき"
      ],
      "metadata": {
        "id": "oB8K9RRoKxDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 疑問符、引用符、ダッシュなどの他の種類の句読点や特殊文字も扱えるようにする\n",
        "text = \"Hello, world. Is this-- a test?\"\n",
        "\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "id": "PRrC2H57LSny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"the-verdict.txt\"に適用\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ],
      "metadata": {
        "id": "J1oJsY75L5k8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 合計のトークン数\n",
        "print(len(preprocessed))"
      ],
      "metadata": {
        "id": "VbYDqaF8MdWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 トークンをトークンIDに変換する"
      ],
      "metadata": {
        "id": "k1UMgu_aNAdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 一意なトークンから成るリストを作成し、アルファベット順に並び替える\n",
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "D-u4H6lJN7Gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 語彙を作成して、最初の51個のエントリを出力\n",
        "vocab = {token:integer for integer, token in enumerate(all_words)}"
      ],
      "metadata": {
        "id": "BW4R4RqON4_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i >= 50:\n",
        "        break"
      ],
      "metadata": {
        "id": "ra2_nNVaOiu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# シンプルなテキストトークナイザーの実装\n",
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab # encodeメソッドとdecodeメソッドでアクセスできるように語彙をクラス属性に格納\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()} # トークンIDからテキストトークンにマッピングするための逆引き辞書\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"入力テキストをトークンIDに変換\"\"\"\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "        preprocessed = [\n",
        "            item for item in preprocessed if item.strip()\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        \"\"\"トークンを変換してテキストに戻す\"\"\"\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # 指定された句読点の前にあるスペースを削除\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "VResJf8TOvXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"The Verdict.txt\"の内容を語彙にテキストをトークン化\n",
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"\"\"\"It's the last he painted, you know,\"\n",
        "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "id": "mgxb9eRPQ9xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# トークンIDからテキストに戻せるか\n",
        "print(tokenizer.decode(ids))"
      ],
      "metadata": {
        "id": "eVXVnj2mRiw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 訓練セット（語彙）に含まれていないテキストでもできるか\n",
        "text = \"Hello, do you like tea?\"\n",
        "print(tokenizer.encode(text))"
      ],
      "metadata": {
        "id": "f2Kq5_yERp9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 語彙に含まれていないのでKeyErrorになる\n",
        "- 語彙を増やすために大規模で多様な訓練データセットを考慮する必要がある"
      ],
      "metadata": {
        "id": "ew9vdNQRR8kK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 特別なコンテキストトークンを追加する\n",
        "- 未知の単語に対処するためにトークナイザーを修正する必要がある\n",
        "- 特別なコンテキストトークンの使い方と、そうしたトークンを追加することについても検討が必要\n",
        "- コンテキストトークンを追加すると、テキストのコンテキストやその他の関連情報に対するモデル理解を向上させることができる\n",
        "- 特別なトークンには、未知の単語や文章の境界を示すマーカーを含めることができる\n",
        "\n",
        "### 特殊トークン\n",
        "- 一部のトークナイザーは、LLMに追加の文脈を提供するために特殊トークンを使用する\n",
        "- これらの特殊トークンには以下のようなものがある\n",
        "- `[BOS]`（シーケンスの開始）はテキストの開始点を示す\n",
        "- `[EOS]`（シーケンスの終了）はテキストの終了点を示す（通常、複数の無関係なテキスト（例. 異なるウィキペディア記事や異なる書籍など）を連結するために使用される）\n",
        "- `[PAD]`（パディング）:バッチサイズが1より大きいLLMを訓練する場合に使用（長さの異なる複数のテキストを含める場合、パディングトークンで短いテキストを最長テキストの長さに調整し、全テキストの長さを均一化する）\n",
        "- `[UNK]`:語彙に含まれない単語を表す\n",
        "\n",
        "- GPT-2は上記のトークンは一切使用せず、複雑さを軽減するために`<|endpftext|>`トークンのみを使用する\n",
        "- `<|endoftext|>`は前述の`[EOS]`トークンに相当する\n",
        "- GPTもパディングに`<|endoftext|>`を使用（バッチ入力で訓練する際は通常マスクを使用し、パディングトークンには注意が向けないため、これらのトークンに内容に問題にならない）\n",
        "- GPT-2は語彙以外に`<UNK>`トークンを使用せず、代わりにバイトペアエンコーディング（BPE）トークナイザーを採用。これは単語をサブワード単位に分解するもの"
      ],
      "metadata": {
        "id": "D5y3M0M2SJBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <|unk|>と<|endoftext|>の2つの特殊トークンを単語リストに追加\n",
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "vocab = {token:integer for integer, token in enumerate(all_tokens)}"
      ],
      "metadata": {
        "id": "vbccxMgZTEov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 元の語彙：1,130\n",
        "len(vocab.items())"
      ],
      "metadata": {
        "id": "5vKm5wafjCPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 更新された語彙の最後の5つ\n",
        "# 追加した2つの特殊トークンが含まれていること確認\n",
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "qUjcLf10jFQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 未知の単語に対処するシンプルなテキストトークナイザー\n",
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int\n",
        "            else \"<|unk|>\" for item in preprocessed\n",
        "        ] # 語彙になけれな\"<|unk|>\"で埋める\n",
        "\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # 指定された句読点の前にあるスペースを置き換える\n",
        "        text = re.sub(r'\\sL([,.:;?!()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "rh2iwzPnjVta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 試してみる\n",
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "print(text)"
      ],
      "metadata": {
        "id": "AsH4xH7ik4Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# トークナイズ&エンコードしてみる\n",
        "tokenizer.encode(text)"
      ],
      "metadata": {
        "id": "Le1KZhJXlQEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# デコード\n",
        "tokenizer.decode(tokenizer.encode(text))"
      ],
      "metadata": {
        "id": "S8H8lFJTlb0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 バイトペアエンコーディング（BPE）\n",
        "- GPT-2はトークナイザーとしてBytePairエンコーディング（BPE）を採用\n",
        "- これによりモデルは、事前定義された語彙にない単語を小さなサブワード単位や個々の文字に分解でき、語彙以外の単語を処理できるようになる\n",
        "- 例えば、GPT-2の語彙に「unfamiliarword」という単語が存在しない場合、学習済みのBPEマージ設定に応じて、これを[「unfam」, [iliar], [word]]などのサブワード分解でトークン化することがある（サブワードが語彙にある前提）"
      ],
      "metadata": {
        "id": "kYmx3jGYlvmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BPEトークナイザーのインスタンス化\n",
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "VfGiQIijnrDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 使い方\n",
        "# トークナイズ&エンコーディング\n",
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "    \"of someunknownPlace.\"\n",
        ")\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(integers)"
      ],
      "metadata": {
        "id": "k6rgJ6qkn46H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# デコード\n",
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "id": "VFy6AcgOoUTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- BPEトークナイザーを使用することで未知の単語をサブワードや個々の文字に分割する\n",
        "- 結果、どのような単語が来ても解析できるため、BPEトークナイザーでは、未知の単語を`<|unk|>`のような特殊トークンで置き換える必要がない -> トークンID振れさえすれば良い -> エンコード ⇔ デコードで対応できれば良い\n",
        "- あらゆるテキストが処理できる"
      ],
      "metadata": {
        "id": "dL1mQAh0ogkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 練習問題 2-1\n",
        "text = \"Akwirw ier\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(f\"トークンID: {ids}\")\n",
        "text_decode = tokenizer.decode(ids)\n",
        "print(f\"デコードテキスト: {text_decode}\")"
      ],
      "metadata": {
        "id": "B9eh-mXopg5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6スライディングウィンドウによるデータサンプリング\n",
        "- 入力変数と目的変数のペアを生成する\n",
        "- LLMの事前学習はテキストにおいて次に来る単語を予測する方法で行われる"
      ],
      "metadata": {
        "id": "7jGOiBV9qJny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BPEトークナイザー使って「The Veedict」をトークン化\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ],
      "metadata": {
        "id": "9gcsKMVV1xIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 最初の50ワードを削除\n",
        "enc_sample = enc_text[50:]"
      ],
      "metadata": {
        "id": "x_c9x_4c2Hnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 次単語予測のため入力変数と目的変数のペアを作成\n",
        "# xには入力トークン、yには（xを1つ後ろにシフトさせた）ターゲットトークン\n",
        "context_size = 4\n",
        "\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:      {y}\")"
      ],
      "metadata": {
        "id": "i5VJaMaB2WGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(context, \"---->\", desired)"
      ],
      "metadata": {
        "id": "cMBNLIQi20se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# トークンIDをテキストに変換したバージョン\n",
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
      ],
      "metadata": {
        "id": "Sl5V8jLg3SCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# バッチ入力と目的変数のためのデータセット\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # トークナイザーで全テキストをトークン化\n",
        "        token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max length+1\"\n",
        "\n",
        "        # スライディングウィンドウを使ってmax_lengthの長さのシーケンスに分割\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "-4zNN3UM3rSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5hpp3mVa53rX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}