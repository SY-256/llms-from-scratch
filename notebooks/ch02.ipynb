{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMXJvxejFAUF5gxqqFhZJMT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SY-256/llms-from-scratch/blob/main/notebooks/ch02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter2: Working with Text Data"
      ],
      "metadata": {
        "id": "bO9Q881eHAKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 テキストをトークン化する"
      ],
      "metadata": {
        "id": "AkWUZIB2N8jC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "print(\"torch version: \", version(\"torch\"))\n",
        "print(\"tiktoken version: \", version(\"tiktoken\"))"
      ],
      "metadata": {
        "id": "wJAmnZV3H4CD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "if not os.path.exists(\"the-verdict.txt\"):\n",
        "    url = (\n",
        "        \"https://raw.githubusercontent.com/rasbt/\"\n",
        "        \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "        \"the-verdict.txt\"\n",
        "    )\n",
        "    file_path = \"the-verdict.txt\"\n",
        "\n",
        "    response = requests.get(url, timeout=30)\n",
        "    response.raise_for_status()\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        f.write(response.content)\n"
      ],
      "metadata": {
        "id": "K_a7i3EXIIzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print(\"Total number of character. \", len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "id": "EFyy_yoDIu1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 空白で文字列分割\n",
        "import re\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "aYN6uRbXJJ2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- テキストをすべて小文字にするのはやめる -> 大文字はLLMが固有名詞と普通名詞を区別し、文の構造を理解し、大文字を正しく使ったテキストの生成を学習するのに役立つため安易に小文字に変換しない"
      ],
      "metadata": {
        "id": "ToTMVqPZJnd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ホワイトスペース+コンマ、ピリオドでの正規表現分割\n",
        "result = re.split(r'([,.]|\\s)', text)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "18RZJJTpKK05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ホワイトスペース不要なので除く\n",
        "item = re.split(r'([,.]|\\s)', text)\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "id": "tc5yrgARKa0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ホワイトスペースを取り除くか／取り除かないかはアプリケーションとその要件によって変わってくので、一概にすべて取り除けばよいという訳でもない\n",
        "- テキストの正確な構造に敏感なモデル（Pythonコード生成）であればホワイトスペースは残すべき"
      ],
      "metadata": {
        "id": "oB8K9RRoKxDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 疑問符、引用符、ダッシュなどの他の種類の句読点や特殊文字も扱えるようにする\n",
        "text = \"Hello, world. Is this-- a test?\"\n",
        "\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "id": "PRrC2H57LSny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"the-verdict.txt\"に適用\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ],
      "metadata": {
        "id": "J1oJsY75L5k8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 合計のトークン数\n",
        "print(len(preprocessed))"
      ],
      "metadata": {
        "id": "VbYDqaF8MdWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 トークンをトークンIDに変換する"
      ],
      "metadata": {
        "id": "k1UMgu_aNAdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 一意なトークンから成るリストを作成し、アルファベット順に並び替える\n",
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "D-u4H6lJN7Gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 語彙を作成して、最初の51個のエントリを出力\n",
        "vocab = {token:integer for integer, token in enumerate(all_words)}"
      ],
      "metadata": {
        "id": "BW4R4RqON4_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i >= 50:\n",
        "        break"
      ],
      "metadata": {
        "id": "ra2_nNVaOiu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# シンプルなテキストトークナイザーの実装\n",
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab # encodeメソッドとdecodeメソッドでアクセスできるように語彙をクラス属性に格納\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()} # トークンIDからテキストトークンにマッピングするための逆引き辞書\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"入力テキストをトークンIDに変換\"\"\"\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "        preprocessed = [\n",
        "            item for item in preprocessed if item.strip()\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        \"\"\"トークンを変換してテキストに戻す\"\"\"\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # 指定された句読点の前にあるスペースを削除\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "VResJf8TOvXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"The Verdict.txt\"の内容を語彙にテキストをトークン化\n",
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"\"\"\"It's the last he painted, you know,\"\n",
        "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "id": "mgxb9eRPQ9xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# トークンIDからテキストに戻せるか\n",
        "print(tokenizer.decode(ids))"
      ],
      "metadata": {
        "id": "eVXVnj2mRiw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 訓練セット（語彙）に含まれていないテキストでもできるか\n",
        "text = \"Hello, do you like tea?\"\n",
        "print(tokenizer.encode(text))"
      ],
      "metadata": {
        "id": "f2Kq5_yERp9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 語彙に含まれていないのでKeyErrorになる\n",
        "- 語彙を増やすために大規模で多様な訓練データセットを考慮する必要がある"
      ],
      "metadata": {
        "id": "ew9vdNQRR8kK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 特別なコンテキストトークンを追加する\n",
        "- 未知の単語に対処するためにトークナイザーを修正する必要がある\n",
        "- 特別なコンテキストトークンの使い方と、そうしたトークンを追加することについても検討が必要\n",
        "- コンテキストトークンを追加すると、テキストのコンテキストやその他の関連情報に対するモデル理解を向上させることができる\n",
        "- 特別なトークンには、未知の単語や文章の境界を示すマーカーを含めることができる\n",
        "\n",
        "### 特殊トークン\n",
        "- 一部のトークナイザーは、LLMに追加の文脈を提供するために特殊トークンを使用する\n",
        "- これらの特殊トークンには以下のようなものがある\n",
        "- `[BOS]`（シーケンスの開始）はテキストの開始点を示す\n",
        "- `[EOS]`（シーケンスの終了）はテキストの終了点を示す（通常、複数の無関係なテキスト（例. 異なるウィキペディア記事や異なる書籍など）を連結するために使用される）\n",
        "- `[PAD]`（パディング）:バッチサイズが1より大きいLLMを訓練する場合に使用（長さの異なる複数のテキストを含める場合、パディングトークンで短いテキストを最長テキストの長さに調整し、全テキストの長さを均一化する）\n",
        "- `[UNK]`:語彙に含まれない単語を表す\n",
        "\n",
        "- GPT-2は上記のトークンは一切使用せず、複雑さを軽減するために`<|endpftext|>`トークンのみを使用する\n",
        "- `<|endoftext|>`は前述の`[EOS]`トークンに相当する\n",
        "- GPTもパディングに`<|endoftext|>`を使用（バッチ入力で訓練する際は通常マスクを使用し、パディングトークンには注意が向けないため、これらのトークンに内容に問題にならない）\n",
        "- GPT-2は語彙以外に`<UNK>`トークンを使用せず、代わりにバイトペアエンコーディング（BPE）トークナイザーを採用。これは単語をサブワード単位に分解するもの"
      ],
      "metadata": {
        "id": "D5y3M0M2SJBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <|unk|>と<|endoftext|>の2つの特殊トークンを単語リストに追加\n",
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "vocab = {token:integer for integer, token in enumerate(all_tokens)}"
      ],
      "metadata": {
        "id": "vbccxMgZTEov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 元の語彙：1,130\n",
        "len(vocab.items())"
      ],
      "metadata": {
        "id": "5vKm5wafjCPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 更新された語彙の最後の5つ\n",
        "# 追加した2つの特殊トークンが含まれていること確認\n",
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "qUjcLf10jFQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 未知の単語に対処するシンプルなテキストトークナイザー\n",
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int\n",
        "            else \"<|unk|>\" for item in preprocessed\n",
        "        ] # 語彙になけれな\"<|unk|>\"で埋める\n",
        "\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # 指定された句読点の前にあるスペースを置き換える\n",
        "        text = re.sub(r'\\sL([,.:;?!()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "rh2iwzPnjVta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 試してみる\n",
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "print(text)"
      ],
      "metadata": {
        "id": "AsH4xH7ik4Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# トークナイズ&エンコードしてみる\n",
        "tokenizer.encode(text)"
      ],
      "metadata": {
        "id": "Le1KZhJXlQEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# デコード\n",
        "tokenizer.decode(tokenizer.encode(text))"
      ],
      "metadata": {
        "id": "S8H8lFJTlb0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 バイトペアエンコーディング（BPE）\n",
        "- GPT-2はトークナイザーとしてBytePairエンコーディング（BPE）を採用\n",
        "- これによりモデルは、事前定義された語彙にない単語を小さなサブワード単位や個々の文字に分解でき、語彙以外の単語を処理できるようになる\n",
        "- 例えば、GPT-2の語彙に「unfamiliarword」という単語が存在しない場合、学習済みのBPEマージ設定に応じて、これを[「unfam」, [iliar], [word]]などのサブワード分解でトークン化することがある（サブワードが語彙にある前提）"
      ],
      "metadata": {
        "id": "kYmx3jGYlvmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BPEトークナイザーのインスタンス化\n",
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "VfGiQIijnrDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 使い方\n",
        "# トークナイズ&エンコーディング\n",
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "    \"of someunknownPlace.\"\n",
        ")\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(integers)"
      ],
      "metadata": {
        "id": "k6rgJ6qkn46H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# デコード\n",
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "id": "VFy6AcgOoUTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- BPEトークナイザーを使用することで未知の単語をサブワードや個々の文字に分割する\n",
        "- 結果、どのような単語が来ても解析できるため、BPEトークナイザーでは、未知の単語を`<|unk|>`のような特殊トークンで置き換える必要がない -> トークンID振れさえすれば良い -> エンコード ⇔ デコードで対応できれば良い\n",
        "- あらゆるテキストが処理できる"
      ],
      "metadata": {
        "id": "dL1mQAh0ogkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 練習問題 2-1\n",
        "text = \"Akwirw ier\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(f\"トークンID: {ids}\")\n",
        "text_decode = tokenizer.decode(ids)\n",
        "print(f\"デコードテキスト: {text_decode}\")"
      ],
      "metadata": {
        "id": "B9eh-mXopg5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6スライディングウィンドウによるデータサンプリング\n",
        "- 入力変数と目的変数のペアを生成する\n",
        "- LLMの事前学習はテキストにおいて次に来る単語を予測する方法で行われる"
      ],
      "metadata": {
        "id": "7jGOiBV9qJny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BPEトークナイザー使って「The Veedict」をトークン化\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ],
      "metadata": {
        "id": "9gcsKMVV1xIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 最初の50ワードを削除\n",
        "enc_sample = enc_text[50:]"
      ],
      "metadata": {
        "id": "x_c9x_4c2Hnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 次単語予測のため入力変数と目的変数のペアを作成\n",
        "# xには入力トークン、yには（xを1つ後ろにシフトさせた）ターゲットトークン\n",
        "context_size = 4\n",
        "\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:      {y}\")"
      ],
      "metadata": {
        "id": "i5VJaMaB2WGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(context, \"---->\", desired)"
      ],
      "metadata": {
        "id": "cMBNLIQi20se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# トークンIDをテキストに変換したバージョン\n",
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
      ],
      "metadata": {
        "id": "Sl5V8jLg3SCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# バッチ入力と目的変数のためのデータセット\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # トークナイザーで全テキストをトークン化\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max length+1\"\n",
        "\n",
        "        # スライディングウィンドウを使ってmax_lengthの長さのシーケンスに分割\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "-4zNN3UM3rSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 入力変数と目的変数のペアでバッチを生成するデータローダー\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                      stride=128, shuffle=True, drop_last=True,\n",
        "                      num_workers=0):\n",
        "    # トークナイザーを初期化\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # データセット作成\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last, # drop_last=Trueは、指定されたbatch_sizeよりも最後のバッチが短い場合に、訓練中の損失値のスパイクを防ぐためにそのバッチを除外\n",
        "        num_workers=num_workers # 前処理に使用するCPU数\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "5hpp3mVa53rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\") as f:\n",
        "    raw_text = f.read()"
      ],
      "metadata": {
        "id": "m8ja4BGa9nMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# コンテキストサイズが4のLLMで、バッチサイズが1のデータローダーをテスト\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "GKM3x2IS91up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "id": "o66AOaED-V_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "secod_batch = next(data_iter)\n",
        "print(secod_batch)"
      ],
      "metadata": {
        "id": "2NZE_g-W-muS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 練習問題 2-2\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=8, stride=2, shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "yQF2UifJ_Kt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "id": "xbzM-BYw_XtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "バッチサイズが小さいと訓練時に必要なメモリが少なくなるものの、モデルの更新時にノイズが増える（勾配の計算やパラメータの更新が不安定になる）\n",
        "\n",
        "バッチサイズはトレードオフのハイパーパラメータ"
      ],
      "metadata": {
        "id": "0osp0EO0_Y9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データローダーで1よりも大きいバッチサイズでサンプリングを行う\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=4, stride=4, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ],
      "metadata": {
        "id": "IAtG7R-kApFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1つのtorch.Tensorにbatch_size分\n",
        "1つの要素はmax_length分\n",
        "\n",
        "オーバーラップが多いと（モデルが同じデータを何度も見てしまい）過剰適合のリスクが高まる"
      ],
      "metadata": {
        "id": "Df1fe20mBZBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.7 トークン埋め込みを作成する\n",
        "- トークンIDを埋め込みベクトルに変換する\n",
        "- 下準備として、埋め込み層の重みをランダムな値で初期化する必要がある\n",
        "- テキストをトークン化 -> テキストトークンをトークンIDに変換 -> トークンIDを埋め込みベクトルに変換する\n",
        "\n",
        "### 連続値のベクトル表現（埋め込み）が必要なのは、GPT型のLLMが誤差逆伝播（バックプロパゲーションアルゴリズム）で訓練されたディープニューラルネットワークだから"
      ],
      "metadata": {
        "id": "tXPpMvcsB0JV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# トークンIDが埋め込みベクトルに変換される仕組みの確認\n",
        "input_ids = torch.tensor([2, 3, 5, 1])"
      ],
      "metadata": {
        "id": "6D0se6d8W4Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6語の小さいな語彙\n",
        "# サイズ3の埋め込みを作成（GPT-3の埋め込みサイズは12,288次元）\n",
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "BXPkgwYxXqk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 埋め込み層の行列\n",
        "print(embedding_layer.weight)"
      ],
      "metadata": {
        "id": "mat7GwhUYFS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "埋め込み層の重み行列には、小さな乱数値が含まれる。これらの値は,、LLM自体の最適化の一部として、LLMの訓練時に最適化される。\n",
        "\n",
        "語彙に含まれるトークン（6つ）ごとに1つの行列があり、埋め込みベクトルの次元（３）ごとに1つの列がある"
      ],
      "metadata": {
        "id": "hKiaZ3s0YKV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 埋め込み層をトークンIDに適用して埋め込みベクトルを作成\n",
        "print(embedding_layer(torch.tensor([3])))"
      ],
      "metadata": {
        "id": "KvB0HXG0YucE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "埋め込み層の重み行列のインデックス4に相当する値が取得できる\n",
        "\n",
        "埋め込み層は事実上、トークンIDに基づいて埋め込み層の重い行列から行を取り出している（ルックアップ演算）"
      ],
      "metadata": {
        "id": "N8KZX_laY7tQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(input_ids))"
      ],
      "metadata": {
        "id": "Z6EH3cueZhRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.8 単語の位置をエンコードする\n",
        "- トークン埋め込みは、基本的にはLLMの入力に適している\n",
        "- LLMの欠点は、そのSelf-Attentionメカニズムにシーケンス内のトークンの位置や順序という概念がないこと\n",
        "- トークンIDがシーケンス内のどの位置にあろうと、同じトークンIDを常に同じベクトル表現にマッピングする\n",
        "- `相対位置埋め込み`と`絶対位置埋め込み`で位置を認識できる仕組みを与えることでトークン間の順序や関係を理解するLLMの能力を向上させることができる"
      ],
      "metadata": {
        "id": "MxdyLzZ-Zo0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50257 #（BPEトークナイザーの語彙数）\n",
        "output_dim = 256\n",
        "\n",
        "toke_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "5mYUcBOla--w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データローダーをインスタンス化\n",
        "max_length = 4\n",
        "\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)"
      ],
      "metadata": {
        "id": "QsaWO3lkblv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ],
      "metadata": {
        "id": "QLZXXHSgcA7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "トークンIDのテンソルは8 x 4次元で、データバッチがそれぞれ4つのトークンを持つ8つのテキストサンプルで構成されている"
      ],
      "metadata": {
        "id": "yA5XaAO_cKz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# トークンIDを256次元のベクトルに埋め込む\n",
        "token_embeddings = toke_embedding_layer(inputs)\n",
        "print(token_embeddings)\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "id": "8vN_OwuLcZjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPTモデルが使っている絶対位置埋め込みの場合は、token_embedding_layerと同じ埋め込み次元を持つ別の埋め込み層を作成すれば良い"
      ],
      "metadata": {
        "id": "8HggH067cqEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
      ],
      "metadata": {
        "id": "ij9dGm9udL10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings)\n",
        "print(pos_embeddings.shape)"
      ],
      "metadata": {
        "id": "SWbZaQz0dduo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 位置埋め込みテンソルは直接トークン埋め込みに追加できる（次元数が一致するから）\n",
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ],
      "metadata": {
        "id": "9bOj0Wo0dnmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.arange(max_length)"
      ],
      "metadata": {
        "id": "6W2fRgpzeQOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embedding_layer.weight"
      ],
      "metadata": {
        "id": "g4G0S7EjeovQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embedding_layer"
      ],
      "metadata": {
        "id": "5Uk8NQl2eurz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iF69Z2aTeyA2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}