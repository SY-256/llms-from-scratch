{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM7gbLrq2YMnTQ/d1hZoX8U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SY-256/llms-from-scratch/blob/main/notebooks/ch02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter2: Working with Text Data"
      ],
      "metadata": {
        "id": "bO9Q881eHAKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 テキストをトークン化する"
      ],
      "metadata": {
        "id": "AkWUZIB2N8jC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "print(\"torch version: \", version(\"torch\"))\n",
        "print(\"tiktoken version: \", version(\"tiktoken\"))"
      ],
      "metadata": {
        "id": "wJAmnZV3H4CD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "if not os.path.exists(\"the-verdict.txt\"):\n",
        "    url = (\n",
        "        \"https://raw.githubusercontent.com/rasbt/\"\n",
        "        \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "        \"the-verdict.txt\"\n",
        "    )\n",
        "    file_path = \"the-verdict.txt\"\n",
        "\n",
        "    response = requests.get(url, timeout=30)\n",
        "    response.raise_for_status()\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        ""
      ],
      "metadata": {
        "id": "K_a7i3EXIIzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print(\"Total number of character. \", len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "id": "EFyy_yoDIu1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 空白で文字列分割\n",
        "import re\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "aYN6uRbXJJ2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- テキストをすべて小文字にするのはやめる -> 大文字はLLMが固有名詞と普通名詞を区別し、文の構造を理解し、大文字を正しく使ったテキストの生成を学習するのに役立つため安易に小文字に変換しない"
      ],
      "metadata": {
        "id": "ToTMVqPZJnd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ホワイトスペース+コンマ、ピリオドでの正規表現分割\n",
        "result = re.split(r'([,.]|\\s)', text)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "18RZJJTpKK05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ホワイトスペース不要なので除く\n",
        "item = re.split(r'([,.]|\\s)', text)\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "id": "tc5yrgARKa0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ホワイトスペースを取り除くか／取り除かないかはアプリケーションとその要件によって変わってくので、一概にすべて取り除けばよいという訳でもない\n",
        "- テキストの正確な構造に敏感なモデル（Pythonコード生成）であればホワイトスペースは残すべき"
      ],
      "metadata": {
        "id": "oB8K9RRoKxDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 疑問符、引用符、ダッシュなどの他の種類の句読点や特殊文字も扱えるようにする\n",
        "text = \"Hello, world. Is this-- a test?\"\n",
        "\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "id": "PRrC2H57LSny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"the-verdict.txt\"に適用\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ],
      "metadata": {
        "id": "J1oJsY75L5k8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 合計のトークン数\n",
        "print(len(preprocessed))"
      ],
      "metadata": {
        "id": "VbYDqaF8MdWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 トークンをトークンIDに変換する"
      ],
      "metadata": {
        "id": "k1UMgu_aNAdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 一意なトークンから成るリストを作成し、アルファベット順に並び替える\n",
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "D-u4H6lJN7Gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 語彙を作成して、最初の51個のエントリを出力\n",
        "vocab = {token:integer for integer, token in enumerate(all_words)}"
      ],
      "metadata": {
        "id": "BW4R4RqON4_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i >= 50:\n",
        "        break"
      ],
      "metadata": {
        "id": "ra2_nNVaOiu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# シンプルなテキストトークナイザーの実装\n",
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab # encodeメソッドとdecodeメソッドでアクセスできるように語彙をクラス属性に格納\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()} # トークンIDからテキストトークンにマッピングするための逆引き辞書\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"入力テキストをトークンIDに変換\"\"\"\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "        preprocessed = [\n",
        "            item for item in preprocessed if item.strip()\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        \"\"\"トークンを変換してテキストに戻す\"\"\"\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # 指定された句読点の前にあるスペースを削除\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "VResJf8TOvXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"The Verdict.txt\"の内容を語彙にテキストをトークン化\n",
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"\"\"\"It's the last he painted, you know,\"\n",
        "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "id": "mgxb9eRPQ9xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# トークンIDからテキストに戻せるか\n",
        "print(tokenizer.decode(ids))"
      ],
      "metadata": {
        "id": "eVXVnj2mRiw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 訓練セット（語彙）に含まれていないテキストでもできるか\n",
        "text = \"Hello, do you like tea?\"\n",
        "print(tokenizer.encode(text))"
      ],
      "metadata": {
        "id": "f2Kq5_yERp9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 語彙に含まれていないのでKeyErrorになる\n",
        "- 語彙を増やすために大規模で多様な訓練データセットを考慮する必要がある"
      ],
      "metadata": {
        "id": "ew9vdNQRR8kK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 特別なコンテキストトークンを追加する\n",
        "- 未知の単語に対処するためにトークナイザーを修正する必要がある\n",
        "- 特別なコンテキストトークンの使い方と、そうしたトークンを追加することについても検討が必要\n",
        "- コンテキストトークンを追加すると、テキストのコンテキストやその他の関連情報に対するモデル理解を向上させることができる\n",
        "- 特別なトークンには、未知の単語や文章の境界を示すマーカーを含めることができる"
      ],
      "metadata": {
        "id": "D5y3M0M2SJBK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vbccxMgZTEov"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}