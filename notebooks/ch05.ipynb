{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPaSP15D9L4myNhWdQQyWfc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SY-256/llms-from-scratch/blob/main/notebooks/ch05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ラベルなしデータでの事前学習\n",
        "- LLMが訓練中に生成したテキストの品質を評価するために、訓練データと検証データセットでの損失（誤差）を計算する。\n",
        "- 訓練関数を実装し、LLMの事前学習を行う。\n",
        "- LLMを引き続き訓練するための重みを保存し、LLMに読み込む。\n",
        "- OpenAIから事前学習済みの重みを読み込む。"
      ],
      "metadata": {
        "id": "mnixJKXGUSro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 生成テキストモデルを評価する"
      ],
      "metadata": {
        "id": "jVUltOjjUaQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPTモデルの設定\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # 語彙のサイズ\n",
        "    \"context_length\": 256, # コンテキストの長さ\n",
        "    \"emb_dim\": 768,         # 埋め込み次元数\n",
        "    \"n_heads\": 12,          # Attentionヘッドの数\n",
        "    \"n_layers\": 12,         # 層の数\n",
        "    \"drop_rate\": 0.1,       # ドロップアウト率\n",
        "    \"qkv_bias\": False       # クエリ、キー、バリューの計算にバイアスを使用するか\n",
        "}"
      ],
      "metadata": {
        "id": "3FPGC7hyVOzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "OM2jJ4VSV1Yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Layer Normalizationの実装と適用をモジュール化\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False) # unbiased=True 有偏分散\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps) # 標準化（ゼロ除算にならないようにepsを加える）\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "# GELU活性化関数の実装\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "# フォードフォワードニューラルネットワークモジュール\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), # 1つ目の線形層で入力の次元から4倍にして出力\n",
        "            GELU(), # GELU活性化関数\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), # 2つ目の線形層で4倍にした入力の次元数を4分の1の元の次元数に戻して出力\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# 3章のMulti-head Attentionコンポーネント\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0) # 出力に次元数がヘッド数と割り切れない場合は警告出す\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # 出力次元数をヘッド数で分割\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "        self.out_proj = nn.Linear(d_out, d_out) # Linear層を使ってヘッドの出力を組み合わせる\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # maskを作成 (context_lengthの形状に従う)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape # テンソルの形状は(batch, num_tokens, d_out)\n",
        "\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # num_heads次元を追加して行列えお暗黙的に分割\n",
        "        # 最後の次元を展開し、形状を(batch, num_tokens, d_out) -> (batch, num_tokens, num_heads, head_dim)に変換\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # 形状を(batch, num_tokens, num_heads, head_dim) ->\n",
        "        # (batch, num_heads, num_tokens, head_dim)に変換\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(2, 3) # 各ヘッドのドット積を計算\n",
        "\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens] # マスクをトークン数で切り捨て\n",
        "\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf) # Attentionスコアを埋めるためにマスクを使う\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2) # テンソルの形状は(batch, num_tokens, n_heads, head_dim)\n",
        "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
        "\n",
        "        context_vec = self.out_proj(context_vec) # 線形射影を追加\n",
        "\n",
        "        return context_vec\n",
        "\n",
        "# GPTアーキテクチャのTransformerブロックコンポーネント\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"]\n",
        "        )\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        shortcut = x # Attentionブロックのショートカット接続(入力のオリジナル)\n",
        "        x = self.norm1(x)\n",
        "        x = self.attn(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut # 元の入力をショートカット接続として入力\n",
        "\n",
        "        shortcut = x # フィードフォワードブロックのショートカット接続(前段の処理結果)\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut\n",
        "\n",
        "        return x\n",
        "\n",
        "# モデルアーキテクチャの実装\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        # Transformerブロック\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
        "        )\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "\n",
        "        # デバイス設定\n",
        "        pos_embeds = self.pos_emb(\n",
        "            torch.arange(seq_len, device=in_idx.device)\n",
        "        )\n",
        "        x = tok_embeds + pos_embeds\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "WGyIMpKdVeDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval();"
      ],
      "metadata": {
        "id": "TQNfYa-LVzyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "コンテキストの長さ（context_length）を1024->256に短縮したことで、モデルの訓練に必要な計算量が少なくなり、標準的なラップトップコンピュータでも計算可能になる"
      ],
      "metadata": {
        "id": "vL4XHcCLWGxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# テキストをトークンIDに変換するユーティリティ関数\n",
        "\n",
        "# テキストを生成するGPTモデルの関数\n",
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:] # サポートされているコンテキストサイズを超える場合は現在のコンテキストを切り詰める\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # unsqueeze(0)はバッチ次元を追加する\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # バッチ次元を削除\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "start_context = \"Every effort moves you\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "print(f\"Output text:\\n {token_ids_to_text(token_ids, tokenizer)}\")"
      ],
      "metadata": {
        "id": "Ho-pZYu_WjhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 損失関数の実装"
      ],
      "metadata": {
        "id": "BvurXwe3X6kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2つの入力サンプルについて考える\n",
        "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
        "                       [40,    1107, 588]])   #  \"I really like\"]\n",
        "\n",
        "# モデルに生成させたいトークンID\n",
        "# inputsを右に1つシフトさせたもの\n",
        "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
        "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
      ],
      "metadata": {
        "id": "KAhcoNLhed19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ロジットベクトルを計算（確率スコア）\n",
        "with torch.no_grad():\n",
        "    logits = model(inputs)\n",
        "\n",
        "probas = torch.softmax(logits, dim=-1) # 語彙の各トークンの確率\n",
        "print(probas.shape)"
      ],
      "metadata": {
        "id": "5LgyHgTDelPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1つ目の数字`2`は、入力の2つのサンプル（行）に対応している。バッチサイズともいう。\n",
        "\n",
        "2つめの数字`3`は、各入力（列）のトークン数。\n",
        "\n",
        "最後の数字は埋め込み次元数であり、語彙のサイズによって決定される。"
      ],
      "metadata": {
        "id": "PpDOxwGbfEtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 確率スコアにargmax()関数を適用して対応するトークンIDを取得\n",
        "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
        "print(f\"Token IDs:\\n{token_ids}\")"
      ],
      "metadata": {
        "id": "8cw7s9S_fc81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# トークンIDをテキストに戻す\n",
        "print(f\"Tragets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
        "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
      ],
      "metadata": {
        "id": "1lZAmUhyfzgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2つの入力テキストそれぞれについて、ターゲットトークンに対応するソフトマックス確率スコアを出力\n",
        "text_idx = 0\n",
        "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
        "print(f\"Text 1: {target_probas_1}\")\n",
        "\n",
        "text_idx = 1\n",
        "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
        "print(f\"Text 2: {target_probas_2}\")"
      ],
      "metadata": {
        "id": "G2lcWEE6gK37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLMの訓練の目的は正しいトークンが生成される確率の最大化"
      ],
      "metadata": {
        "id": "KGj62L6ChReF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 対数確率\n",
        "# 最適化の観点から、確率スコアをそのまま使用するよりも対数にした方が扱いやすい\n",
        "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
        "print(log_probas)"
      ],
      "metadata": {
        "id": "DIgR4DCWha0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 対数確率の平均を求めて、1つのスコアにまとめる\n",
        "avg_log_probas = torch.mean(log_probas)\n",
        "print(avg_log_probas)"
      ],
      "metadata": {
        "id": "JXl3uxbwh3_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "訓練プロセスの一部としてモデルの重みを更新しながら、この平均対数確率をできるだけ0に近づけることが目標となる。\n",
        "\n",
        "ディープラーニングでは、平均対数確率を直接0に近づけるのではなく、負の平均対数確率を0に近づけるのが一般的\n",
        "\n",
        "`負の対数確率`: 単に平均対数確率に`-1`をかけた値のこと"
      ],
      "metadata": {
        "id": "xLgFXujeiTBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 負の対数確率を求める\n",
        "neg_avg_log_probas = avg_log_probas * -1\n",
        "print(neg_avg_log_probas)"
      ],
      "metadata": {
        "id": "dtHCMCWSiybT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 交差エントロピー誤差\n",
        "2つの確率分布の差を定量的に計測する指標であり、機械学習やディープラーニングでよく使われる。モデルが生成したトークン確率に基づく、ターゲットトークンの負の平均対数確率とほぼ同じもの。"
      ],
      "metadata": {
        "id": "wbILKfRKi53o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ロジットテンソルとターゲットテンソルの形状を確認\n",
        "print(f\"Logits shape: {logits.shape}\")\n",
        "print(f\"Targets shape: {targets.shape}\")"
      ],
      "metadata": {
        "id": "7HqIwfCZjb5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logitsテンソル -> (バッチサイズ、トークン数、語彙のサイズ)\n",
        "\n",
        "Targetsテンソル -> (バッチサイズ、トークン数)\n",
        "\n",
        "cross_entropy()損失関数を使用する場合には、これらのテンソルをバッチ次元で結合することでフラット化( faltten() )しておく必要がある。"
      ],
      "metadata": {
        "id": "0nA3BsumjoFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits_flat = logits.flatten(0, 1)\n",
        "target_flat = targets.flatten()\n",
        "print(f\"Flattened logits: {logits_flat.shape}\")\n",
        "print(f\"Flattened targets: {target_flat.shape}\")"
      ],
      "metadata": {
        "id": "jGxOruvLkBYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorchのCrossEntropy関数はこれまでのすべての作業を自動化でやってくれる\n",
        "# softmax() -> ターゲットIDに対する確率スコア選択 -> 負の平均対数確率計算\n",
        "loss = torch.nn.functional.cross_entropy(logits_flat, target_flat)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "FyD7UhWceO6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### パープレキシティ\n",
        "- 言語モデルのようなタスクでモデルの性能評価する際に、交差エントロピー誤差と並んで使われる指標\n",
        "- シーケンスの次に来るトークンを予測するモデルの不確かさを、より解釈しやすい方法で理解するための手段となる\n",
        "- モデルが予測した確率分布が、データセット内の単語の実際の分布とどの程度一致するかを計測する\n",
        "- 損失(誤差)と同様に、パープレキシティが低いほど、モデルの予測が実際の分布に近いことを意味する\n",
        "- しばしば損失関数よりも解釈がしやすいとみなされるのは、各ステップでモデルが不確かとなる実質的な語彙のサイズを示すからであるパープレキシティが`tensor(46725.8203)`の場合、「語彙に含まれる48,725個のトークンのうち、次に来るトークンとしてどれを生成すべきかについてモデルは確信を持てない」と解釈できる"
      ],
      "metadata": {
        "id": "fmAltozaeh3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 訓練データと検証データで損失を計算する\n",
        "# 訓練用、検証用データの準備\n",
        "import os\n",
        "import requests\n",
        "\n",
        "file_path = \"the-verdict.txt\"\n",
        "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    response = requests.get(url, timeout=30)\n",
        "    response.raise_for_status()\n",
        "    text_data = response.text\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(text_data)\n",
        "\n",
        "else:\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        text_data = file.read()"
      ],
      "metadata": {
        "id": "ugWYSa3Cf-90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データセット内の文字数とトークン数を確認\n",
        "total_characters = len(text_data)\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "print(f\"Characters: {total_characters}\")\n",
        "print(f\"Tokens: {total_tokens}\")"
      ],
      "metadata": {
        "id": "gTpoJ4w2hkKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLMを可変長の入力で訓練するのも効果的 -> 様々な種類の入力にLLMを上手く汎用化させるのに役立つ"
      ],
      "metadata": {
        "id": "QsPChnkzh2hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データのシャッフルと読み込みの実装\n",
        "train_ration = 0.90\n",
        "split_idx = int(train_ration * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]"
      ],
      "metadata": {
        "id": "5_sApyjAiWPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# バッチ入力と目的変数のためのデータセット\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # トークナイザーで全テキストをトークン化\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max length+1\"\n",
        "\n",
        "        # スライディングウィンドウを使ってmax_lengthの長さのシーケンスに分割\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "sjHvtCdoio7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 入力変数と目的変数のペアでバッチを生成するデータローダー\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                      stride=128, shuffle=True, drop_last=True,\n",
        "                      num_workers=0):\n",
        "    # トークナイザーを初期化\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # データセット作成\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last, # drop_last=Trueは、指定されたbatch_sizeよりも最後のバッチが短い場合に、訓練中の損失値のスパイクを防ぐためにそのバッチを除外\n",
        "        num_workers=num_workers # 前処理に使用するCPU数\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "Go3h-bb-i260"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 訓練データセットと検証データセットのデータローダーを作成\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ],
      "metadata": {
        "id": "cjB-PZzZi4QM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データローダーが問題なく作成されたか確認\n",
        "print(\"Train loader: \")\n",
        "for x, y in train_loader:\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "print(\"Validation loader: \")\n",
        "for x, y in val_loader:\n",
        "    print(x.shape, y.shape)"
      ],
      "metadata": {
        "id": "LwnB31Eajfuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 交差エントロピー誤差を計算するユーティリティ関数を実装\n",
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch = input_batch.to(device)\n",
        "    target_batch = target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(\n",
        "        logits.flatten(0, 1), target_batch.flatten()\n",
        "    )\n",
        "\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "AvaJQM3Fjv8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 訓練と検証の損失を計算する関数\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        # num_batchesが指定されていない場合\n",
        "        # すべてのバッチを反復処理\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # num_batchesがデータローダーのバッチ数を超えている場合、データローダーのバッチ数と一致するように調節\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(\n",
        "                input_batch, target_batch, model, device\n",
        "            )\n",
        "            total_loss += loss.item() # 各バッチの損失を計算\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return total_loss / num_batches # 全バッチの損失の平均を計算 (バッチ総数に対する損失の平均値)"
      ],
      "metadata": {
        "id": "dAoFafl9kYot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calc_loss_loader()を訓練データセットと検証データセットに適用\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device) # GPU搭載であればLLMの訓練をGPUで行う\n",
        "\n",
        "with torch.no_grad():\n",
        "    # まだ訓練していないため、効率化のために勾配の追跡を無効にする\n",
        "    train_loss = calc_loss_loader(train_loader, model, device)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device)\n",
        "\n",
        "print(f\"Training loss: {train_loss}\")\n",
        "print(f\"Validation loss: {val_loss}\")"
      ],
      "metadata": {
        "id": "6li9YHNFlylL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "損失が大きいのはLLMがまだ訓練を行っていないから\n",
        "\n",
        "訓練データセットと検証データセットに次に現れるトークンを生成するようにモデルを訓練した場合、損失値は0に近づいていく"
      ],
      "metadata": {
        "id": "40Iv2mVF3LNa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLMを訓練する"
      ],
      "metadata": {
        "id": "q-vJR-kO3gUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LLMの事前学習を行うためのメイン関数\n",
        "def train_model_simple(model, train_loader, val_loader, optimizer, device,\n",
        "                       num_epochs, eval_freq, eval_iter, start_context, tokenizer):\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], [] # 損失と既視のトークンを追跡するためにリストを初期化\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train() # 学習モード\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # 前回のバッチ反復処理で計算された損失の勾配をリセット\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # 損失勾配を計算\n",
        "            optimizer.step() # 損失の勾配を使ってモデルの重みを更新\n",
        "            tokens_seen += input_batch.numel()\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "            if global_step % eval_freq == 0: # オプションの評価ステップ\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter\n",
        "                )\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): Train loss {train_loss:.3f} Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # 各エポックの後にサンプルテキストを出力\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen\n",
        "\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    # モデルを更新するたびに訓練データセットと検証データセットでの損失を出力\n",
        "    model.eval() # 評価モード\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(\n",
        "            train_loader, model, device, num_batches=eval_iter\n",
        "        )\n",
        "        val_loss = calc_loss_loader(\n",
        "            val_loader, model, device, num_batches=eval_iter\n",
        "        )\n",
        "\n",
        "    model.train()\n",
        "    return train_loss, val_loss\n",
        "\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    # 訓練中にモデルが改善されたかどうかを追跡する関数\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded, max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \")) # コンパクトな出力フォーマット\n",
        "    model.train()"
      ],
      "metadata": {
        "id": "qGa5n0hG3oqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AdamW\n",
        "Adamオプティマイザの改良版\n",
        "\n",
        "大きな重みにペナルティを課すことでモデルの複雑さを最小限に抑え、過剰適合を防ぐことを目的として、重みの減衰の処理方法が改善されている。この調整によってより効率的な正則化が可能になり、モデルの汎用性能が向上することから、AdamWはLLMの訓練に非常によく使われている。"
      ],
      "metadata": {
        "id": "AsXvuN0H7omd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPTModelインスタンスを10エポックにわたって訓練\n",
        "torch.manual_seed(123)\n",
        "\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=0.0004, weight_decay=0.1 # 重み減衰\n",
        ")\n",
        "\n",
        "num_epochs = 10\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "LENRUMzm8Jgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import integer\n",
        "from matplotlib.lines import lineStyles\n",
        "# 損失についてプロット\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(nrows=1, ncols=1, figsize=(5, 3))\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "\n",
        "    ax2 = ax1.twiny() # 同じy軸を共有する2つ目のx軸を作成\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0) # メモリを揃えるための不可視プロット\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "id": "eDUlEi9A85ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "モデルがデータに対して過剰適合している\n",
        "\n",
        "モデルが訓練データの文章をそのまま記憶していることがわかる -> 非常に小さい訓練データセットを使用して複数回エポックを回しているため\n",
        "\n",
        "本来はもっと大きなデータセットで1エポックだけ訓練するのが一般的"
      ],
      "metadata": {
        "id": "JFVzk7f6_Grf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 ランダム性をコントロールするデコーディング戦略"
      ],
      "metadata": {
        "id": "Kfxmq0x8_hJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(\"cpu\")\n",
        "model.eval()\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
        "    max_new_tokens=25,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "print(f\"Output text:\\n {token_ids_to_text(token_ids, tokenizer)}\")"
      ],
      "metadata": {
        "id": "PI4GHb1s_-qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Temperature Scalling\n",
        "# 確率的サンプリング\n",
        "vocab = {\n",
        "    \"closer\": 0,\n",
        "    \"every\": 1,\n",
        "    \"effort\": 2,\n",
        "    \"forward\": 3,\n",
        "    \"inches\": 4,\n",
        "    \"moves\": 5,\n",
        "    \"pizza\": 6,\n",
        "    \"toward\": 7,\n",
        "    \"you\": 8,\n",
        "}\n",
        "\n",
        "inverse_vocab = {v: k for k, v in vocab.items()}\n",
        "\n",
        "next_token_logits = torch.tensor(\n",
        "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
        ")\n",
        "\n",
        "probas = torch.softmax(next_token_logits, dim=0)\n",
        "next_token_id = torch.argmax(probas).item()\n",
        "\n",
        "print(inverse_vocab[next_token_id])"
      ],
      "metadata": {
        "id": "8Dt8Z_B-ZXGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 確率的サンプリングを実装するために、torch.multinominal()関数に置き換え\n",
        "torch.manual_seed(123)\n",
        "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
        "print(inverse_vocab[next_token_id])"
      ],
      "metadata": {
        "id": "fNFlroCNyxaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.multinomial()関数は、次に来るトークンをその確率スコアに比例する形でサンプリングする\n",
        "\n",
        "ただし、確率スコアによるサンプリングのため毎回選択されるわけではない"
      ],
      "metadata": {
        "id": "oDMd0yaizNr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 100回繰り返す\n",
        "def print_sampled_tokens(probas):\n",
        "    torch.manual_seed(123)\n",
        "    sample = [torch.multinomial(probas, num_samples=1).item()\n",
        "                for i in range(1_000)]\n",
        "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
        "    for i, freq in enumerate(sampled_ids):\n",
        "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
        "\n",
        "print_sampled_tokens(probas)"
      ],
      "metadata": {
        "id": "HGQXcDa4zgv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 温度スケーリング: 0よりも大きい数でロジットを割ることのもったいぶった言い方"
      ],
      "metadata": {
        "id": "05FsVCNS0DsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_with_temperature(logits, temperature):\n",
        "    scaled_logits = logits / temperature\n",
        "    return torch.softmax(scaled_logits, dim=0)\n",
        "\n",
        "temperatures = [1, 0.1, 5]\n",
        "\n",
        "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
      ],
      "metadata": {
        "id": "qRp_is1x0bBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 異なる温度設定でスケーリングした確率を並べる\n",
        "x = torch.arange(len(vocab))\n",
        "bar_width = 0.15\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5, 3))\n",
        "for i, T in enumerate(temperatures):\n",
        "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f\"Temperature = {T}\")\n",
        "\n",
        "ax.set_ylabel(\"Probability\")\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"tempareture-plot.pdf\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lLrXjMv0010x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tempareture=1: 温度スケーリングを使用しないことと同じ\n",
        "\n",
        "温度設定を高くすれば、生成されるテキストの多様性を高めることができるが、意味不明なテキストが生成されるケースも多くなる"
      ],
      "metadata": {
        "id": "NDCx6KJG1oUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# top-kサンプリング\n",
        "# 選択の対象となるトークンを最も高い上位k個のトークンに限定できる\n",
        "# それ以外のトークンについては、それらの確率スコアをマスクすることで選択プロセスから除外できる（対象外の選択を負の無限大(-inf)でマスク）\n",
        "\n",
        "top_k = 3\n",
        "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
        "print(f\"Top logits: {top_logits}\")\n",
        "print(f\"Top positions: {top_pos}\")"
      ],
      "metadata": {
        "id": "nfDi1Tjl2S2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ロジットの大きさが上位3つに含まれないトークンのロジットを負の無限大（-inf）に設定\n",
        "new_logits = torch.where(\n",
        "    condition=next_token_logits < top_logits[-1], # 上位3つのロジットよりも小さいロジットを特定\n",
        "    input=torch.tensor(float(\"-inf\")), # 上位3つ以外を-infに設定\n",
        "    other=next_token_logits # それ以外はそのまま\n",
        ")\n",
        "print(new_logits)"
      ],
      "metadata": {
        "id": "OHR9mufraJ1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 次に来るトークンの確率に変換\n",
        "topk_probas = torch.softmax(new_logits, dim=0)\n",
        "print(topk_probas)"
      ],
      "metadata": {
        "id": "JYVSdHxKbdIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 多様性を高める新しいテキスト生成関数\n",
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        if top_k is not None:\n",
        "            topk_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = topk_logits[:, -1]\n",
        "            logits = torch.where(\n",
        "                logits < min_val,\n",
        "                torch.tensor(float('-inf')).to(logits.device),\n",
        "                logits\n",
        "            )\n",
        "\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "            logits = logits - logits.max(dim=-1, keepdim=True).values\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            # 温度スケーリングが無効の場合は従来同様に貪欲なデコーディング\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "        # シーケンス終了トークンが検出\n",
        "        # 生成を早期終了\n",
        "        if idx_next == eos_id:\n",
        "            break\n",
        "\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "PIvmS40SbqXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用してみる\n",
        "torch.manual_seed(123)\n",
        "\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(\"cpu\"),\n",
        "    max_new_tokens=15,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
        "    top_k=25,\n",
        "    temperature=1.4\n",
        ")\n",
        "\n",
        "print(f\"Output text:\\n {token_ids_to_text(token_ids, tokenizer)}\")"
      ],
      "metadata": {
        "id": "9koFsFvyddiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4 PyTorchでモデルの重みの保存と読み込み"
      ],
      "metadata": {
        "id": "xjcD1KJjd8UL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルパラメータの保存\n",
        "# \".pth\"はPyTorchファイルの慣例的な拡張子 -> 厳密にはどんな拡張子使っても良い\n",
        "torch.save(model.state_dict(), \"model.pth\")"
      ],
      "metadata": {
        "id": "nAD_rRAygtH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPTModelインスタンスで保存したモデル情報の読み込み\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "MfvK9a5fwthQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AdamWのような適応型オプティマイザは、各モデルの重みに追加のパラメータを格納する\n",
        "\n",
        "AdamWは過去のデータをもとに各モデルパラメータの学習率を動的に調整する\n",
        "\n",
        "このようにしないと、オプティマイザがリセットされ、モデルの学習が最適でなくなったり、場合によっては上手く収束しなくなったりして、一貫性のあるテキストを生成する能力がそこなわれるためである"
      ],
      "metadata": {
        "id": "dQpEOWcrw9fG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルとオプティマイザの情報を保存\n",
        "torch.save({\n",
        "    \"model_state_dict\": model.state_dict(),\n",
        "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "},\n",
        "    \"model_and_optimizer.pth\")"
      ],
      "metadata": {
        "id": "t7c3YGiRxvdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルとオプティマイザを保存した情報から読み込み\n",
        "checkpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
        "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "model.train()"
      ],
      "metadata": {
        "id": "Tgao4FOUyI1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.5 OpenAIから事前学習済みの重みを読み込む\n",
        "- Linear層、Embedding層のweight属性に格納されている重みパラメータをOpenAIが公開している情報を使用して読み込む"
      ],
      "metadata": {
        "id": "WiUe6uhpyWti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow>=2.15.0 tqdm>=4.66"
      ],
      "metadata": {
        "id": "PNvgT-_8z166"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# バージョン確認\n",
        "from importlib.metadata import version\n",
        "\n",
        "print(f\"Tensorflow version: {version(\"tensorflow\")}\")\n",
        "print(f\"tqdm version: {version(\"tqdm\")}\")"
      ],
      "metadata": {
        "id": "LdM0U-UQ0bfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
        "# Source for \"Build a Large Language Model From Scratch\"\n",
        "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
        "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def download_and_load_gpt2(model_size, models_dir):\n",
        "    # Validate model size\n",
        "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
        "    if model_size not in allowed_sizes:\n",
        "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
        "\n",
        "    # Define paths\n",
        "    model_dir = os.path.join(models_dir, model_size)\n",
        "    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
        "    backup_base_url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2\"\n",
        "    filenames = [\n",
        "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
        "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
        "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
        "    ]\n",
        "\n",
        "    # Download files\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    for filename in filenames:\n",
        "        file_url = os.path.join(base_url, model_size, filename)\n",
        "        backup_url = os.path.join(backup_base_url, model_size, filename)\n",
        "        file_path = os.path.join(model_dir, filename)\n",
        "        download_file(file_url, file_path, backup_url)\n",
        "\n",
        "    # Load settings and params\n",
        "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
        "    settings = json.load(open(os.path.join(model_dir, \"hparams.json\"), \"r\", encoding=\"utf-8\"))\n",
        "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
        "\n",
        "    return settings, params\n",
        "\n",
        "\n",
        "def download_file(url, destination, backup_url=None):\n",
        "    def _attempt_download(download_url):\n",
        "        response = requests.get(download_url, stream=True, timeout=60)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        file_size = int(response.headers.get(\"Content-Length\", 0))\n",
        "\n",
        "        # Check if file exists and has same size\n",
        "        if os.path.exists(destination):\n",
        "            file_size_local = os.path.getsize(destination)\n",
        "            if file_size and file_size == file_size_local:\n",
        "                print(f\"File already exists and is up-to-date: {destination}\")\n",
        "                return True\n",
        "\n",
        "        block_size = 1024  # 1 KB\n",
        "        desc = os.path.basename(download_url)\n",
        "        with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=desc) as progress_bar:\n",
        "            with open(destination, \"wb\") as file:\n",
        "                for chunk in response.iter_content(chunk_size=block_size):\n",
        "                    if chunk:\n",
        "                        file.write(chunk)\n",
        "                        progress_bar.update(len(chunk))\n",
        "        return True\n",
        "\n",
        "    try:\n",
        "        if _attempt_download(url):\n",
        "            return\n",
        "    except requests.exceptions.RequestException:\n",
        "        if backup_url is not None:\n",
        "            print(f\"Primary URL ({url}) failed. Attempting backup URL: {backup_url}\")\n",
        "            try:\n",
        "                if _attempt_download(backup_url):\n",
        "                    return\n",
        "            except requests.exceptions.RequestException:\n",
        "                pass\n",
        "\n",
        "        error_message = (\n",
        "            f\"Failed to download from both primary URL ({url})\"\n",
        "            f\"{' and backup URL (' + backup_url + ')' if backup_url else ''}.\"\n",
        "            \"\\nCheck your internet connection or the file availability.\\n\"\n",
        "            \"For help, visit: https://github.com/rasbt/LLMs-from-scratch/discussions/273\"\n",
        "        )\n",
        "        print(error_message)\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Alternative way using `requests`\n",
        "\"\"\"\n",
        "def download_file(url, destination):\n",
        "    # Send a GET request to download the file in streaming mode\n",
        "    response = requests.get(url, stream=True)\n",
        "\n",
        "    # Get the total file size from headers, defaulting to 0 if not present\n",
        "    file_size = int(response.headers.get(\"content-length\", 0))\n",
        "\n",
        "    # Check if file exists and has the same size\n",
        "    if os.path.exists(destination):\n",
        "        file_size_local = os.path.getsize(destination)\n",
        "        if file_size == file_size_local:\n",
        "            print(f\"File already exists and is up-to-date: {destination}\")\n",
        "            return\n",
        "\n",
        "    # Define the block size for reading the file\n",
        "    block_size = 1024  # 1 Kilobyte\n",
        "\n",
        "    # Initialize the progress bar with total file size\n",
        "    progress_bar_description = url.split(\"/\")[-1]  # Extract filename from URL\n",
        "    with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description) as progress_bar:\n",
        "        # Open the destination file in binary write mode\n",
        "        with open(destination, \"wb\") as file:\n",
        "            # Iterate over the file data in chunks\n",
        "            for chunk in response.iter_content(block_size):\n",
        "                progress_bar.update(len(chunk))  # Update progress bar\n",
        "                file.write(chunk)  # Write the chunk to the file\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
        "    # Initialize parameters dictionary with empty blocks for each layer\n",
        "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
        "\n",
        "    # Iterate over each variable in the checkpoint\n",
        "    for name, _ in tf.train.list_variables(ckpt_path):\n",
        "        # Load the variable and remove singleton dimensions\n",
        "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
        "\n",
        "        # Process the variable name to extract relevant parts\n",
        "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
        "\n",
        "        # Identify the target dictionary for the variable\n",
        "        target_dict = params\n",
        "        if variable_name_parts[0].startswith(\"h\"):\n",
        "            layer_number = int(variable_name_parts[0][1:])\n",
        "            target_dict = params[\"blocks\"][layer_number]\n",
        "\n",
        "        # Recursively access or create nested dictionaries\n",
        "        for key in variable_name_parts[1:-1]:\n",
        "            target_dict = target_dict.setdefault(key, {})\n",
        "\n",
        "        # Assign the variable array to the last key\n",
        "        last_key = variable_name_parts[-1]\n",
        "        target_dict[last_key] = variable_array\n",
        "\n",
        "    return params"
      ],
      "metadata": {
        "id": "5bF7K_cI0xy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # GitHubのリポジトリから読み込む\n",
        "# import urllib.request\n",
        "\n",
        "# url = (\"https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/01_main-chapter-code/gpt_download.py\")\n",
        "# filename = url.split(\"/\")[-1]\n",
        "# urllib.request.urlretrieve(url, filename)"
      ],
      "metadata": {
        "id": "iiBMbB5--akD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "settings, params = download_and_load_gpt2(\n",
        "    model_size=\"124M\",\n",
        "    models_dir=\"gpt2\"\n",
        ")"
      ],
      "metadata": {
        "id": "6H4uRSjA-0pZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# settingsとparamsの内容を確認\n",
        "print(f\"Settings: {settings}\")\n",
        "print(f\"Parameter dectionary keys: {params.keys()}\")"
      ],
      "metadata": {
        "id": "SmZ9EMGB_vs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# トークンの埋め込み層の重みを表示\n",
        "print(params[\"wte\"])\n",
        "print(f\"Token embedding weight tensor dimenstions: {params[\"wte\"].shape}\")"
      ],
      "metadata": {
        "id": "tFn5n1BJBDI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-2モデルアーキテクチャのパラメータ数の違い\n",
        "\n",
        "基本的なアーキテクチャは同じ\n",
        "\n",
        "個々のコンポーネント（Attentionヘッド、Transformerブロックなど）の繰り返し回数と埋め込みサイズが異なる"
      ],
      "metadata": {
        "id": "tU0M9fL1Be2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの設定\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "# モデルパラメータの設定を更新\n",
        "model_name = \"gpt2-small (124M)\"\n",
        "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
        "NEW_CONFIG.update(model_configs[model_name])\n",
        "\n",
        "# OpenAIのモデルはコンテキストの長さが1,024でバイアスを使っている\n",
        "# 一貫性を保つために、これらのパラメータ内容を更新して、設定を一致させる\n",
        "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
        "\n",
        "gpt = GPTModel(NEW_CONFIG)\n",
        "gpt.eval()"
      ],
      "metadata": {
        "id": "pajZjF1RB8Ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPTModelインスタンスは事前学習のためにランダムな重みで初期化される\n",
        "# OpenAIのモデルの重みを使うための最後のステップは、このランダムな重みをparamsディクショナリに読み込んだ重みで上書きする\n",
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismath. Left: {left.shape}, Right: {right.shape}\")\n",
        "    # 形状が同じであれば訓練可能なPyTorchパラメータとして、rightテンソルを返す\n",
        "    return torch.nn.Parameter(torch.tensor(right))"
      ],
      "metadata": {
        "id": "sNiHpGEPDANT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
        "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
        "\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].attn.W_query.weight = assign(\n",
        "            gpt.trf_blocks[b].attn.W_query.weight, q_w.T)\n",
        "        gpt.trf_blocks[b].attn.W_key.weight = assign(\n",
        "            gpt.trf_blocks[b].attn.W_key.weight, k_w.T)\n",
        "        gpt.trf_blocks[b].attn.W_value.weight = assign(\n",
        "            gpt.trf_blocks[b].attn.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].attn.W_query.bias = assign(\n",
        "            gpt.trf_blocks[b].attn.W_query.bias, q_b)\n",
        "        gpt.trf_blocks[b].attn.W_key.bias = assign(\n",
        "            gpt.trf_blocks[b].attn.W_key.bias, k_b)\n",
        "        gpt.trf_blocks[b].attn.W_value.bias = assign(\n",
        "            gpt.trf_blocks[b].attn.W_value.bias, v_b)\n",
        "\n",
        "        gpt.trf_blocks[b].attn.out_proj.weight = assign(\n",
        "            gpt.trf_blocks[b].attn.out_proj.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].attn.out_proj.bias = assign(\n",
        "            gpt.trf_blocks[b].attn.out_proj.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].norm1.scale = assign(\n",
        "            gpt.trf_blocks[b].norm1.scale,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm1.shift = assign(\n",
        "            gpt.trf_blocks[b].norm1.shift,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.trf_blocks[b].norm2.scale = assign(\n",
        "            gpt.trf_blocks[b].norm2.scale,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm2.shift = assign(\n",
        "            gpt.trf_blocks[b].norm2.shift,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
        "\n",
        "\n",
        "load_weights_into_gpt(gpt, params)\n",
        "gpt.to(device);"
      ],
      "metadata": {
        "id": "C292QvHyEGJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load_weight_info_gpt()関数では、OpenAIの実装とGPTModelの実装の重みを慎重にマッチさせている。"
      ],
      "metadata": {
        "id": "lmLiV7cKrj81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAIモデルの重みをGPTModelのインスタンスgptに読み込み\n",
        "load_weights_into_gpt(gpt, params)\n",
        "gpt.to(device)"
      ],
      "metadata": {
        "id": "i0jtpg_MtJi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt.trf_blocks[0].attn"
      ],
      "metadata": {
        "id": "YKAzAJPCuw-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "token_ids = generate(\n",
        "    model=gpt,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
        "    max_new_tokens=25,\n",
        "    context_size=NEW_CONFIG[\"context_length\"],\n",
        "    top_k=50,\n",
        "    temperature=1.5\n",
        ")\n",
        "print(f\"Output text:\\n{token_ids_to_text(token_ids, tokenizer)}\")"
      ],
      "metadata": {
        "id": "qkizHXuCtWle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 練習問題 5-5\n",
        "# 事前学習済みの重みを適用したモデルを使用して損失を計算\n",
        "\n",
        "# calc_loss_loader()を訓練データセットと検証データセットに適用\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "gpt.to(device) # GPU搭載であればLLMの訓練をGPUで行う\n",
        "\n",
        "with torch.no_grad():\n",
        "    # まだ訓練していないため、効率化のために勾配の追跡を無効にする\n",
        "    train_loss = calc_loss_loader(train_loader, gpt, device)\n",
        "    val_loss = calc_loss_loader(val_loader, gpt, device)\n",
        "\n",
        "print(f\"Training loss: {train_loss}\")\n",
        "print(f\"Validation loss: {val_loss}\")"
      ],
      "metadata": {
        "id": "3jvYvKNGxH_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 練習問題 5-6\n",
        "# 1.5Bのモデルで生成したテキストの内容確認\n",
        "# モデルの設定\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "# モデルパラメータの設定を更新\n",
        "model_name = \"gpt2-xl (1558M)\"\n",
        "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
        "NEW_CONFIG.update(model_configs[model_name])\n",
        "\n",
        "# OpenAIのモデルはコンテキストの長さが1,024でバイアスを使っている\n",
        "# 一貫性を保つために、これらのパラメータ内容を更新して、設定を一致させる\n",
        "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
        "\n",
        "gpt_1558m = GPTModel(NEW_CONFIG)\n",
        "gpt_1558m.eval()"
      ],
      "metadata": {
        "id": "nCuMhWx7yoHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI側\n",
        "settings, params = download_and_load_gpt2(\n",
        "    model_size=\"1558M\",\n",
        "    models_dir=\"gpt2\"\n",
        ")"
      ],
      "metadata": {
        "id": "Rzppz0SpzhY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_weights_into_gpt(gpt_1558m, params)\n",
        "gpt_1558m.to(device);"
      ],
      "metadata": {
        "id": "T9jrsCJ6zO5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# テキスト生成\n",
        "torch.manual_seed(123)\n",
        "\n",
        "token_ids = generate(\n",
        "    model=gpt_1558m,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
        "    max_new_tokens=25,\n",
        "    context_size=NEW_CONFIG[\"context_length\"],\n",
        "    top_k=50,\n",
        "    temperature=1.5\n",
        ")\n",
        "print(f\"Output text:\\n{token_ids_to_text(token_ids, tokenizer)}\")"
      ],
      "metadata": {
        "id": "Ea4eF5X58ayt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 練習問題 5-5\n",
        "# 事前学習済みの重みを適用したモデルを使用して損失を計算\n",
        "\n",
        "# calc_loss_loader()を訓練データセットと検証データセットに適用\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "gpt_1558m.to(device) # GPU搭載であればLLMの訓練をGPUで行う\n",
        "\n",
        "with torch.no_grad():\n",
        "    # まだ訓練していないため、効率化のために勾配の追跡を無効にする\n",
        "    train_loss = calc_loss_loader(train_loader, gpt_1558m, device)\n",
        "    val_loss = calc_loss_loader(val_loader, gpt_1558m, device)\n",
        "\n",
        "print(f\"Training loss: {train_loss}\")\n",
        "print(f\"Validation loss: {val_loss}\")"
      ],
      "metadata": {
        "id": "qWxX8qT-8iZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "82mBIwJo8wZt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}