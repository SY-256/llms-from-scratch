{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMvdXH4LwsX174fQAbIS7EX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SY-256/llms-from-scratch/blob/main/notebooks/ch06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 分類のためのファインチューニング\n",
        "- LLMのさまざまなファインチューニングアプローチ\n",
        "- スパムメールを識別するために事前学習済みLLMをファインチューニングする"
      ],
      "metadata": {
        "id": "uB3dcSbgAbh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 ファインチューニングのさまざまなカテゴリ\n",
        "- インストラクションチューニング: 特定の指示を使用した一連のタスクを言語モデルに訓練することで、自然言語のプロンプトで表示されたタスクを理解して実行する能力を向上させる\n",
        "- 分類チューニング: 特定のクラスラベルを認識する能力を向上させる\n",
        "\n",
        "インストラクションチューニングを行ったモデルは、幅広いタスクに対応できる\n",
        "\n",
        "分類チューニングを行ったモデルは、訓練中に遭遇したクラスの予測に限定される（専門性が高い）"
      ],
      "metadata": {
        "id": "FPEiAu_eAu50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 データセットを準備する"
      ],
      "metadata": {
        "id": "PkrYIKWFHeCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データセットのダウンロードと解凍\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "zip_path = \"sms_spam_collection.zip\"\n",
        "extracted_path = \"sms_spam_collection\"\n",
        "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
        "\n",
        "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
        "    if data_file_path.exists():\n",
        "        print(f\"{data_file_path} already exists. Skipping download and extraction\")\n",
        "        return\n",
        "\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "        # ファイルダウンロード\n",
        "        with open(zip_path, \"wb\") as out_file:\n",
        "            out_file.write(response.read())\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "        # ファイル解凍\n",
        "        zip_ref.extractall(extracted_path)\n",
        "\n",
        "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
        "    os.rename(original_file_path, data_file_path) # ファイル拡張子.tsvを追加\n",
        "    print(f\"File downloaded and saved as {data_file_path}\")\n",
        "\n",
        "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
      ],
      "metadata": {
        "id": "Ub2nBQ8xHwru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データの読み込み\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\n",
        "    data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"]\n",
        ")\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "m69lWCwHJNDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# クラスラベルの分布\n",
        "print(df[\"Label\"].value_counts())"
      ],
      "metadata": {
        "id": "DByuo6a3JfLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# アンダーサンプリングして均衡なデータセットを作成\n",
        "def create_balanced_dataset(df):\n",
        "    # スパムの数に合わせてデータセットをアンダーサンプリング\n",
        "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
        "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(\n",
        "        num_spam, random_state=123\n",
        "    )\n",
        "    balanced_df = pd.concat(\n",
        "        [ham_subset, df[df[\"Label\"] == \"spam\"]]\n",
        "    )\n",
        "\n",
        "    return balanced_df\n",
        "\n",
        "balanced_df = create_balanced_dataset(df)\n",
        "print(balanced_df[\"Label\"].value_counts())"
      ],
      "metadata": {
        "id": "3LdMNo9yJoIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ラベルのマッピング\n",
        "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})"
      ],
      "metadata": {
        "id": "ifq4frnRKeO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データセットを訓練／検証／評価用に分割する\n",
        "def random_split(df, train_frac, validation_frac):\n",
        "\n",
        "    df = df.sample(\n",
        "        frac=1, random_state=123\n",
        "    ).reset_index(drop=True)\n",
        "    train_end = int(len(df) * train_frac) # 分割インデックスを計算\n",
        "    validation_end = train_end + int(len(df) * validation_frac)\n",
        "\n",
        "    train_df = df[:train_end] # DataFrameを分割\n",
        "    validation_df = df[train_end:validation_end]\n",
        "    test_df = df[validation_end:]\n",
        "\n",
        "    return train_df, validation_df, test_df\n",
        "\n",
        "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)"
      ],
      "metadata": {
        "id": "5tp1AXyeKp6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CSVファイルで保存\n",
        "train_df.to_csv(\"train.csv\", index=None)\n",
        "validation_df.to_csv(\"validation.csv\", index=None)\n",
        "test_df.to_csv(\"test.csv\", index=None)"
      ],
      "metadata": {
        "id": "Q5Tv6_AULjm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3 データローダーを作成する"
      ],
      "metadata": {
        "id": "vk0cS5JsLzXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "テキストの長さがまちまちのテキストチャンク化戦略\n",
        "- すべてのメッセージをデータセットまたはバッチ内で最も短いメッセージと同じ長さに切りそろえる\n",
        "\n",
        "->計算量は少なくなるが、大量の情報が失われる場合がある\n",
        "\n",
        "- すべてのメッセージをデータセットまたはバッチ内で最も長いメッセージと同じ長さにパディングする\n",
        "\n",
        "->パディングトークンとして`\"<|endoftext|>\"`を使用する"
      ],
      "metadata": {
        "id": "p0FotJu1L8Fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# テキストのパディング\n",
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
      ],
      "metadata": {
        "id": "1vkR1mk36lLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch Datasetクラスをセットアップする\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SpamDataset(Dataset):\n",
        "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256): # token_id=50256='<|endoftext|>'のパディングトークン\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.encoded_texts = [\n",
        "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
        "        ]\n",
        "\n",
        "        if max_length is None:\n",
        "            self.max_length = self._longest_encoded_length()\n",
        "        else:\n",
        "            self.max_length = max_length\n",
        "\n",
        "            self.encoded_texts = [\n",
        "                encoded_text[:self.max_length] for encoded_text in self.encoded_texts\n",
        "            ]\n",
        "\n",
        "        self.encoded_texts = [\n",
        "            encoded_text + [pad_token_id] *\n",
        "            (self.max_length - len(encoded_text))\n",
        "            for encoded_text in self.encoded_texts\n",
        "        ]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        encoded = self.encoded_texts[index]\n",
        "        label = self.data.iloc[index][\"Label\"]\n",
        "        return (torch.tensor(encoded, dtype=torch.long),\n",
        "                torch.tensor(label, dtype=torch.long)\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def _longest_encoded_length(self):\n",
        "        max_length = 0\n",
        "        for encoded_text in self.encoded_texts:\n",
        "            encoded_length = len(encoded_text)\n",
        "            if encoded_length > max_length:\n",
        "                max_length = encoded_length\n",
        "\n",
        "        return max_length\n"
      ],
      "metadata": {
        "id": "wkm6q2YY61Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# パディングを適用する\n",
        "train_dataset = SpamDataset(\n",
        "    csv_file=\"train.csv\",\n",
        "    max_length=None,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "p_2BYxs69LqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 最も長いシーケンスのトークン数\n",
        "print(train_dataset.max_length)"
      ],
      "metadata": {
        "id": "Nx3QqU3Z9XWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "最大で1,024トークン（コンテキストの長さの上限値）のシーケンスで対処できる\n",
        "\n",
        "1,024よりも長いテキストがデータセットに含まれている場合、max_length=1024を指定すると1,024を超えないようにできる"
      ],
      "metadata": {
        "id": "DXzcCi_y9fAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 検証データセット／テストデータセットの作成\n",
        "# max_lengthをtrain_dataset.max_lengthに合わせる\n",
        "# 検証データセットとテストデータセットに1,024トークンを超えるシーケンスが存在しない場合は、両方のデータセットでmax_length=Noneを設定できる\n",
        "\n",
        "val_dataset = SpamDataset(\n",
        "    csv_file=\"validation.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "test_dataset = SpamDataset(\n",
        "    csv_file=\"test.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "0EF4jliM93cP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 練習問題 6-1 コンテキストの長さの上限を引き上げる（120 -> 1,024）\n",
        "train_dataset_1024 = SpamDataset(\n",
        "    csv_file=\"train.csv\",\n",
        "    max_length=1024,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(train_dataset_1024.max_length)"
      ],
      "metadata": {
        "id": "kkWY1f1W-gYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset_1024 = SpamDataset(\n",
        "    csv_file=\"validation.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "test_dataset_1024 = SpamDataset(\n",
        "    csv_file=\"test.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "hdMvNIjJ_Rhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorchデータローダーを作成する\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "num_workers = 0 # ほとんどのコンピュータとの互換性を確保する設定\n",
        "batch_size = 8\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False\n",
        "\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False\n",
        ")"
      ],
      "metadata": {
        "id": "Ak-HS7am_VnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 最後のバッチのテンソル次元を出力して、期待されるバッチサイズを実際に返すことを確認\n",
        "for input_batch, target_batch in train_loader:\n",
        "    pass\n",
        "\n",
        "print(f\"Input batch dimensions: {input_batch.shape}\")\n",
        "print(f\"Label batch dimensions: {target_batch.shape}\")"
      ],
      "metadata": {
        "id": "HhqP0QiWPhgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 各データセットのバッチの総数を出力\n",
        "print(f\"{len(train_loader)} training batches\")\n",
        "print(f\"{len(val_loader)} validation batches\")\n",
        "print(f\"{len(test_loader)} test batches\")\n"
      ],
      "metadata": {
        "id": "ILFaBORpP9HA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.4 事前学習済みモデルの重みでモデルを初期化する"
      ],
      "metadata": {
        "id": "e6cr3EOdQP_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデル設定\n",
        "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
        "INPUT_PROMPT = \"Every effort moves\"\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257, # 語彙のサイズ\n",
        "    \"context_length\": 1024, # コンテキストの長さ\n",
        "    \"drop_rate\": 0.0, # ドロップアウト率\n",
        "    \"qkv_bias\": True, # クエリ、キーバリューの計算にバイアスを使用するか\n",
        "}\n",
        "\n",
        "model_config = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"ebm_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25}\n",
        "}\n",
        "\n",
        "BASE_CONFIG.update(model_config[CHOOSE_MODEL])\n",
        "\n",
        "assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"], (\n",
        "    f\"Dataset length {train_dataset.max_length} exceeds model's context\"\n",
        "    f\"length {BASE_CONFIG['context_length']}. Reinitialize data sets with\"\n",
        "    f\"`max_length={BASE_CONFIG['context_length']}`\"\n",
        ")"
      ],
      "metadata": {
        "id": "rzZ4ym6oQZl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/rasbt/LLMs-from-scratch.git"
      ],
      "metadata": {
        "id": "u_D25mH6T-jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/LLMs-from-scratch/ch06/01_main-chapter-code')"
      ],
      "metadata": {
        "id": "p162NNwqUJML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 事前学習済みGPTモデルを読み込む\n",
        "from gpt_download import download_and_load_gpt2\n",
        "from previous_chapters import GPTModel, load_weights_into_gpt\n",
        "\n",
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
        "settings, params = download_and_load_gpt2(\n",
        "    model_size=model_size, models_dir=\"gpt2\"\n",
        ")\n",
        "\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "load_weights_into_gpt(model, params) # 各種重みパラメータ設定\n",
        "model.eval();"
      ],
      "metadata": {
        "id": "mnnPotTYSpHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# テキスト生成ユーティリティ関数を再利用\n",
        "from previous_chapters import (\n",
        "    generate_text_simple,\n",
        "    text_to_token_ids,\n",
        "    token_ids_to_text\n",
        ")\n",
        "\n",
        "text_1 = \"Every effort moves you\"\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(text_1, tokenizer),\n",
        "    max_new_tokens=15,\n",
        "    context_size=BASE_CONFIG[\"context_length\"]\n",
        ")\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "id": "Z-1bjRSbStOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ファインチューイング前にモデルに指示を与えてスパムを分類できるか\n",
        "text_2 = (\n",
        "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
        "    \" 'You are a winner you have been specially\"\n",
        "    \" selected to receive $1000 cach or a $2000 award.'\"\n",
        ")\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(text_2, tokenizer),\n",
        "    max_new_tokens=23,\n",
        "    context_size=BASE_CONFIG[\"context_length\"]\n",
        ")\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "id": "xnkd3Jz2WigS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "出力の結果から、モデルが指示に従うのに苦戦していることがわかる\n",
        "\n",
        "モデルは事前学習は行っているが、スパム分類を行う用にインストラクションチューニングは行っていないため、この結果になっている"
      ],
      "metadata": {
        "id": "iwywv6mOXog4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.5 分類ヘッドを追加する\n",
        "- 隠れ層の表現を50,257語の語彙にマッピングする元の出力層を、2つのクラス（`spam`:1 or `not spam`:0）にマッピングする小さな出力層に変える\n",
        "- 出力層を書き換えること以外は、以前のモデルと同じ"
      ],
      "metadata": {
        "id": "I3gTxbLbYCCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 選択した層のファインチューニング vs. すべての層のファインチューニング\n",
        "事前学習済みモデルを使用する場合、一般的にモデルのすべての層を新たにファインチューニングする必要はない。ニューラルネットワークの言語モデルでは、低い方（入力に近い方）の層は一般的に幅広いタスクやデータセットに適用できる基本的な言語構造やセマンティックを捉える。\n",
        "したがって、多くの場合では、言語上の微妙なパターンやタスク固有の特徴量だけに特化した最後の層（出力に近い層）だけをファインチューニングすれば十分である\n",
        "\n",
        "小数の層だけをファインチューニングすることで計算効率も良くなる\n"
      ],
      "metadata": {
        "id": "DCX2zD8nYc2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルを分類チューニングするために、モデルを凍結(freeze)する\n",
        "# モデルを凍結させるとすべての層が訓練不能になる\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "twVhlu3ub3Bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 出力層(model.out_head)を置き換える\n",
        "# 分類層を追加する\n",
        "torch.manual_seed(123)\n",
        "\n",
        "num_classes = 2\n",
        "model.out_head = torch.nn.Linear(\n",
        "    in_features=BASE_CONFIG[\"emb_dim\"],\n",
        "    out_features=num_classes # 今回の分類タスクのクラス数で設定\n",
        ")"
      ],
      "metadata": {
        "id": "U4CHZnpMcMC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "出力層model.out_headのrequires_grad属性はデフォルトでTrueに設定\n",
        "\n",
        "この層はモデルにおいて訓練中に更新される唯一の層になる\n",
        "\n",
        "追加の層もファインチューニングするとモデルの予測精度が向上することがわかっている\n",
        "->最後の正規化層(LayerNorm)、最後のTransformerブロックを訓練したほうが、出力層だけを訓練するより性能が向上する"
      ],
      "metadata": {
        "id": "VHGOKNQ-clQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 最後のLayerNormと最後のTransformerブロックを訓練可能に設定する -> requires_grad=Trueに設定\n",
        "for param in model.trf_blocks[-1].parameters():\n",
        "    param.requires_grad = True # 最後のTransformerブロックを訓練可能に\n",
        "\n",
        "for param in model.final_norm.parameters():\n",
        "    param.requires_grad = True # 最後のLayerNormを訓練可能に"
      ],
      "metadata": {
        "id": "vfK0p1IVdJaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 特定層の訓練可否設定の変更後もこれまでと同じように使用できる\n",
        "inputs = tokenizer.encode(\"Do you have time\")\n",
        "inputs = torch.tensor(inputs).unsqueeze(0)\n",
        "print(f\"Inputs: {inputs}\")\n",
        "print(f\"Inputs dimensions: {inputs.shape}\")"
      ],
      "metadata": {
        "id": "PDAg5nxdd1bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# エンコードされたトークンIDも同じように渡せる\n",
        "with torch.no_grad():\n",
        "    outputs = model(inputs)\n",
        "\n",
        "print(f\"Outputs:\\n {outputs}\")\n",
        "print(f\"Outputs dimensions: {outputs.shape}\")"
      ],
      "metadata": {
        "id": "kRJOAP3gge5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "モデルの出力層を置き換えているので出力テンソル形状が50257 -> 2になっている"
      ],
      "metadata": {
        "id": "415u_SPcgw9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2値分類の場合、すべての出力行をファインチューニングする必要はない\n",
        "# たった1つ（最後の出力トークン）に焦点を合わせることができる\n",
        "# 最後のトークンは、他のすべてのトークンに対するAttentionスコアを持つ唯一のトークンであるため\n",
        "# (バッチサイズ, 入力数, ラベル数)\n",
        "print(f\"Last output token: {outputs[:, -1, :]}\")"
      ],
      "metadata": {
        "id": "ORWl_5K0g82N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.6 分類の損失と正解率を算出する\n",
        "- ファインチューニングで使用する評価関数の実装"
      ],
      "metadata": {
        "id": "MU6AQKpjhg9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 最後の出力トークンを確率に変換する方法\n",
        "print(f\"Last output token: {outputs[:, -1, :]}\")\n",
        "\n",
        "# クラスラベルの取得（softmax関数でlogit -> argmaxでラベル取得）\n",
        "probas = torch.softmax(outputs[:, -1, :], dim=-1)\n",
        "label = torch.argmax(probas)\n",
        "print(f\"Class label: {label.item()}\")"
      ],
      "metadata": {
        "id": "cLmP11SmiKqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 値が大きくなればsoftmax関数の出力も大きくなるので、必ずしもsoftmax関数使わなくても良い\n",
        "logits = outputs[:, -1, :]\n",
        "label = torch.argmax(logits)\n",
        "print(f\"Class label: {label.item()}\")"
      ],
      "metadata": {
        "id": "mrl8cywFizHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 分類正解率を計算する\n",
        "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
        "    model.eval()\n",
        "    correct_predictions, num_examples = 0.0, 0\n",
        "\n",
        "    if num_batches is None:\n",
        "        num_bacthes = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            input_batch = input_batch.to(device)\n",
        "            target_batch = target_batch.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = model(input_batch)[:, -1, :] # 最後の出力トークンのロジットを計算\n",
        "\n",
        "            predicted_labels = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            num_examples += predicted_labels.shape[0]\n",
        "            correct_predictions += (\n",
        "                (predicted_labels == target_batch).sum().item()\n",
        "            )\n",
        "        else:\n",
        "            break\n",
        "        return correct_predictions / num_examples"
      ],
      "metadata": {
        "id": "nBCCuiSNjR5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 分類正解率を計算する関数を使ってみる\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "train_accuracy = calc_accuracy_loader(\n",
        "    train_loader, model, device, num_batches=10\n",
        ")\n",
        "val_accuracy = calc_accuracy_loader(\n",
        "    val_loader, model, device, num_batches=10\n",
        ")\n",
        "test_accuracy = calc_accuracy_loader(\n",
        "    test_loader, model, device, num_batches=10\n",
        ")\n",
        "\n",
        "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "0P8sVWq2kpMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 損失関数の定義\n",
        "# 出力層の結果に対する交差エントロピー誤差を使用\n",
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch = input_batch.to(device)\n",
        "    target_batch = target_batch.to(device)\n",
        "    logits = model(input_batch)[:, -1, :] # 最後の出力トークンのロジット\n",
        "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "np8-Vp8ll80G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データローダから得られるすべてのについて損失を計算する関数\n",
        "# 分類損失を計算する\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # バッチ数がデータローダーのバッチ数を超えないように調節\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(\n",
        "                input_batch, target_batch, model, device\n",
        "            )\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return total_loss / num_batches"
      ],
      "metadata": {
        "id": "EHSyRWZOmxp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 損失計算\n",
        "with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(\n",
        "        train_loader, model, device, num_batches=5\n",
        "    )\n",
        "    val_loss = calc_loss_loader(\n",
        "        val_loader, model, device, num_batches=5\n",
        "    )\n",
        "    test_loss = calc_loss_loader(\n",
        "        test_loader, model, device, num_batches=5\n",
        "    )\n",
        "\n",
        "print(f\"Training loss: {train_loss:.3f}\")\n",
        "print(f\"Validation loss: {val_loss:.3f}\")\n",
        "print(f\"Test loss: {test_loss:.3f}\")"
      ],
      "metadata": {
        "id": "oI7uxvQTnvZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.7 教師ありデータでのモデルファインチューニング\n",
        "- 訓練関数を定義して損失が小さくなるように訓練させる\n",
        "- モデル評価時はテキスト生成ではなく、分類正解率を計算する"
      ],
      "metadata": {
        "id": "ES81V-YVpSkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# スパムを分類するためのモデルのファインチューニング\n",
        "def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter):\n",
        "    # 損失と既視のサンプルを追跡するためにリストを初期化\n",
        "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "    example_seen, global_step = 0, -1\n",
        "\n",
        "    # メインの訓練ループ\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # 勾配リセット\n",
        "            loss = calc_loss_batch(\n",
        "                input_batch, target_batch, model, device\n",
        "            )\n",
        "            loss.backward() # 誤差逆伝播（損失の勾配計算）\n",
        "            optimizer.step() # 損失の勾配を使ってモデルの重みを更新\n",
        "            example_seen += input_batch.shape[0] # トークンではなくサンプルを追跡\n",
        "            global_step += 1\n",
        "\n",
        "            if global_step % eval_freq == 0:\n",
        "                # オプションの評価ステップ\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter\n",
        "                )\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                f\"Train loss {train_loss:.3f}, \"\n",
        "                f\"Val loss {val_loss:.3f}\")\n",
        "\n",
        "        train_accuracy = calc_accuracy_loader(\n",
        "            # 各エポックの後に正解率を計算\n",
        "            train_loader, model, device, num_batches=eval_iter\n",
        "        )\n",
        "        val_accuracy = calc_accuracy_loader(\n",
        "            val_loader, model, device, num_batches=eval_iter\n",
        "        )\n",
        "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
        "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "        train_accs.append(train_accuracy)\n",
        "        val_accs.append(val_accuracy)\n",
        "\n",
        "    return train_losses, val_losses, train_accs, val_accs, example_seen\n",
        "\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(\n",
        "            train_loader, model, device, num_batches=eval_iter\n",
        "        )\n",
        "        val_loss = calc_loss_loader(\n",
        "            val_loader, model, device, num_batches=eval_iter\n",
        "        )\n",
        "\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ],
      "metadata": {
        "id": "H8kB_xaqpuZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 訓練を開始\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "torch.manual_seed(123)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "num_epochs = 5\n",
        "\n",
        "train_losses, val_losses, train_accs, val_accs, example_seen = \\\n",
        "    train_classifier_simple(\n",
        "        model, train_loader, val_loader, optimizer, device, num_epochs=num_epochs, eval_freq=50, eval_iter=5\n",
        "    )\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "id": "ANjRtHA7mUNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 分類損失をプロット\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_values(epochs_seen, example_seen, train_values, val_values, label=\"loss\"):\n",
        "    fig, ax1 = plt.subplots(nrows=1, ncols=1, figsize=(5, 3))\n",
        "\n",
        "    # 各エポックに対する訓練と検証の損失をプロット\n",
        "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
        "    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(label.capitalize())\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2 = ax1.twiny()\n",
        "    ax2.plot(example_seen, train_values, alpha=0)\n",
        "    ax2.set_xlabel(\"Example seen\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.savefig(f\"{label}-plot.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "example_seen_tensor = torch.linspace(0, example_seen, len(train_losses))\n",
        "\n",
        "plot_values(epochs_tensor, example_seen_tensor, train_losses, val_losses)"
      ],
      "metadata": {
        "id": "4i6XZvHBnIpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "損失曲線のプロットから訓練のlossと検証のlossが問題なく推移しているため、過剰適合の兆候もほとんどなく、上手く学習が進んでいることがわかる"
      ],
      "metadata": {
        "id": "1MQTNVWUpNex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## エポック数の選択\n",
        "- エポック数はデータセットとタスクの難易度によって決まる\n",
        "- 通常は5くらいから始めると良い\n",
        "- 最初の数エポックでモデルが過剰適合に陥っていることを損失プロットが示唆している場合は、エポック数を減らすことが考えられる\n",
        "- さらに訓練すると検証データセットの損失が改善する場合はエポック数を増やす\n",
        "- 検証データセットでの損失が0に近いことがベストなエポック数の選択基準になる"
      ],
      "metadata": {
        "id": "DdWORXt1rQSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 分類正解率のプロット\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
        "example_seen_tensor = torch.linspace(0, example_seen, len(train_accs))\n",
        "\n",
        "plot_values(\n",
        "    epochs_tensor, example_seen_tensor, train_accs, val_accs, label=\"accuracy\"\n",
        ")"
      ],
      "metadata": {
        "id": "-ILt4Smmr9EI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "エポックを通じて実線（Train）と破線（Validation）が近接していることは、このモデルが訓練データセットに過剰適合していないことを示唆している"
      ],
      "metadata": {
        "id": "g7Gnbq0OsUJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データセット全体での性能指標を計算\n"
      ],
      "metadata": {
        "id": "vddxd7lvsq9i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}