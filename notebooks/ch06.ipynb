{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNzE77dGh00eAE7xt9iZvsp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SY-256/llms-from-scratch/blob/main/notebooks/ch06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 分類のためのファインチューニング\n",
        "- LLMのさまざまなファインチューニングアプローチ\n",
        "- スパムメールを識別するために事前学習済みLLMをファインチューニングする"
      ],
      "metadata": {
        "id": "uB3dcSbgAbh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 ファインチューニングのさまざまなカテゴリ\n",
        "- インストラクションチューニング: 特定の指示を使用した一連のタスクを言語モデルに訓練することで、自然言語のプロンプトで表示されたタスクを理解して実行する能力を向上させる\n",
        "- 分類チューニング: 特定のクラスラベルを認識する能力を向上させる\n",
        "\n",
        "インストラクションチューニングを行ったモデルは、幅広いタスクに対応できる\n",
        "\n",
        "分類チューニングを行ったモデルは、訓練中に遭遇したクラスの予測に限定される（専門性が高い）"
      ],
      "metadata": {
        "id": "FPEiAu_eAu50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 データセットを準備する"
      ],
      "metadata": {
        "id": "PkrYIKWFHeCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データセットのダウンロードと解凍\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "zip_path = \"sms_spam_collection.zip\"\n",
        "extracted_path = \"sms_spam_collection\"\n",
        "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
        "\n",
        "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
        "    if data_file_path.exists():\n",
        "        print(f\"{data_file_path} already exists. Skipping download and extraction\")\n",
        "        return\n",
        "\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "        # ファイルダウンロード\n",
        "        with open(zip_path, \"wb\") as out_file:\n",
        "            out_file.write(response.read())\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "        # ファイル解凍\n",
        "        zip_ref.extractall(extracted_path)\n",
        "\n",
        "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
        "    os.rename(original_file_path, data_file_path) # ファイル拡張子.tsvを追加\n",
        "    print(f\"File downloaded and saved as {data_file_path}\")\n",
        "\n",
        "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
      ],
      "metadata": {
        "id": "Ub2nBQ8xHwru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データの読み込み\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\n",
        "    data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"]\n",
        ")\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "m69lWCwHJNDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# クラスラベルの分布\n",
        "print(df[\"Label\"].value_counts())"
      ],
      "metadata": {
        "id": "DByuo6a3JfLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# アンダーサンプリングして均衡なデータセットを作成\n",
        "def create_balanced_dataset(df):\n",
        "    # スパムの数に合わせてデータセットをアンダーサンプリング\n",
        "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
        "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(\n",
        "        num_spam, random_state=123\n",
        "    )\n",
        "    balanced_df = pd.concat(\n",
        "        [ham_subset, df[df[\"Label\"] == \"spam\"]]\n",
        "    )\n",
        "\n",
        "    return balanced_df\n",
        "\n",
        "balanced_df = create_balanced_dataset(df)\n",
        "print(balanced_df[\"Label\"].value_counts())"
      ],
      "metadata": {
        "id": "3LdMNo9yJoIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ラベルのマッピング\n",
        "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})"
      ],
      "metadata": {
        "id": "ifq4frnRKeO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データセットを訓練／検証／評価用に分割する\n",
        "def random_split(df, train_frac, validation_frac):\n",
        "\n",
        "    df = df.sample(\n",
        "        frac=1, random_state=123\n",
        "    ).reset_index(drop=True)\n",
        "    train_end = int(len(df) * train_frac) # 分割インデックスを計算\n",
        "    validation_end = train_end + int(len(df) * validation_frac)\n",
        "\n",
        "    train_df = df[:train_end] # DataFrameを分割\n",
        "    validation_df = df[train_end:validation_end]\n",
        "    test_df = df[validation_end:]\n",
        "\n",
        "    return train_df, validation_df, test_df\n",
        "\n",
        "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)"
      ],
      "metadata": {
        "id": "5tp1AXyeKp6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CSVファイルで保存\n",
        "train_df.to_csv(\"train.csv\", index=None)\n",
        "validation_df.to_csv(\"validation.csv\", index=None)\n",
        "test_df.to_csv(\"test.csv\", index=None)"
      ],
      "metadata": {
        "id": "Q5Tv6_AULjm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3 データローダーを作成する"
      ],
      "metadata": {
        "id": "vk0cS5JsLzXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "テキストの長さがまちまちのテキストチャンク化戦略\n",
        "- すべてのメッセージをデータセットまたはバッチ内で最も短いメッセージと同じ長さに切りそろえる\n",
        "\n",
        "->計算量は少なくなるが、大量の情報が失われる場合がある\n",
        "\n",
        "- すべてのメッセージをデータセットまたはバッチ内で最も長いメッセージと同じ長さにパディングする\n",
        "\n",
        "->パディングトークンとして`\"<|endoftext|>\"`を使用する"
      ],
      "metadata": {
        "id": "p0FotJu1L8Fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# テキストのパディング\n",
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
      ],
      "metadata": {
        "id": "1vkR1mk36lLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch Datasetクラスをセットアップする\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SpamDataset(Dataset):\n",
        "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256): # token_id=50256='<|endoftext|>'のパディングトークン\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.encoded_texts = [\n",
        "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
        "        ]\n",
        "\n",
        "        if max_length is None:\n",
        "            self.max_length = self._longest_encoded_length()\n",
        "        else:\n",
        "            self.max_length = max_length\n",
        "\n",
        "            self.encoded_texts = [\n",
        "                encoded_text[:self.max_length] for encoded_text in self.encoded_texts\n",
        "            ]\n",
        "\n",
        "        self.encoded_texts = [\n",
        "            encoded_text + [pad_token_id] *\n",
        "            (self.max_length - len(encoded_text))\n",
        "            for encoded_text in self.encoded_texts\n",
        "        ]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        encoded = self.encoded_texts[index]\n",
        "        label = self.data.iloc[index][\"Label\"]\n",
        "        return (torch.tensor(encoded, dtype=torch.long),\n",
        "                torch.tensor(label, dtype=torch.long)\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def _longest_encoded_length(self):\n",
        "        max_length = 0\n",
        "        for encoded_text in self.encoded_texts:\n",
        "            encoded_length = len(encoded_text)\n",
        "            if encoded_length > max_length:\n",
        "                max_length = encoded_length\n",
        "\n",
        "        return max_length\n"
      ],
      "metadata": {
        "id": "wkm6q2YY61Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# パディングを適用する\n",
        "train_dataset = SpamDataset(\n",
        "    csv_file=\"train.csv\",\n",
        "    max_length=None,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "p_2BYxs69LqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 最も長いシーケンスのトークン数\n",
        "print(train_dataset.max_length)"
      ],
      "metadata": {
        "id": "Nx3QqU3Z9XWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "最大で1,024トークン（コンテキストの長さの上限値）のシーケンスで対処できる\n",
        "\n",
        "1,024よりも長いテキストがデータセットに含まれている場合、max_length=1024を指定すると1,024を超えないようにできる"
      ],
      "metadata": {
        "id": "DXzcCi_y9fAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 検証データセット／テストデータセットの作成\n",
        "# max_lengthをtrain_dataset.max_lengthに合わせる\n",
        "# 検証データセットとテストデータセットに1,024トークンを超えるシーケンスが存在しない場合は、両方のデータセットでmax_length=Noneを設定できる\n",
        "\n",
        "val_dataset = SpamDataset(\n",
        "    csv_file=\"validation.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "test_dataset = SpamDataset(\n",
        "    csv_file=\"test.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "0EF4jliM93cP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 練習問題 6-1 コンテキストの長さの上限を引き上げる（120 -> 1,024）\n",
        "train_dataset_1024 = SpamDataset(\n",
        "    csv_file=\"train.csv\",\n",
        "    max_length=1024,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(train_dataset_1024.max_length)"
      ],
      "metadata": {
        "id": "kkWY1f1W-gYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset_1024 = SpamDataset(\n",
        "    csv_file=\"validation.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "test_dataset_1024 = SpamDataset(\n",
        "    csv_file=\"test.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "hdMvNIjJ_Rhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ak-HS7am_VnO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}